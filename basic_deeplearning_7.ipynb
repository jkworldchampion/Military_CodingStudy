{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "basic_deeplearning_7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPMi830RH8q8ctM8lLGFaxk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkworldchampion/Military_CodingStudy/blob/main/basic_deeplearning_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 합성곱 신경망(CNN)"
      ],
      "metadata": {
        "id": "Jif5qm3pqiku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번 장의 주제는 합성곱 신경망입니다.  \n",
        " CNN은 이미지 인식과 음성 인식 등 다양한 곳에서 사용되는데,   \n",
        " 특히 이미지 인식 분야에서 딥러닝을 활용한 기법은 대부분 CNN을 기반으로 합니다.   \n",
        " 이번장에서는 CNN의 매커니즘을 자세히 알아봅시다."
      ],
      "metadata": {
        "id": "eSc6hgZAqhx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1 전체 구조\n",
        "CNN도 지금까지와 같이 레고 블럭처럼 조합하여 만들 수 있습니다. 다만, 합성곱 계층과 풀링계층이 새롭게 등장합니다. 지금까지의 신경망은 인접하는 계층의 모든 뉴런과 결합되어 있습니다. 이를 완전연결(Fully-Connected)이라고 하며, Affine 계층이라는 이름으로 구현했습니다. 완전연결 신경망은 Affine 계층 뒤에 활성화 함수를 갖는 ReLU 계층이 이어집니다.  \n",
        "이러한 연결이 Conv-ReLU-(Pooling)을 바뀌었다고 생각하면 됩니다. "
      ],
      "metadata": {
        "id": "iF8fMsLYrACw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM1mT5yvs9hL",
        "outputId": "d7a28cab-a6e9-4260-ed7c-16a48f9612d7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Colab_Notebooks/Machine_learning/book/basic_deeplearning/deep-learning-from-scratch/ch07"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3CDWzNJtB1p",
        "outputId": "05c32b24-0d16-4a97-e6dc-c9ec26adf45d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab_Notebooks/Machine_learning/book/basic_deeplearning/deep-learning-from-scratch/ch07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from simple_convnet import SimpleConvNet\n",
        "from matplotlib.image import imread\n",
        "from common.layers import *\n",
        "import pickle\n",
        "from collections import OrderedDict\n",
        "from common.gradient import numerical_gradient\n",
        "from dataset.mnist import load_mnist\n",
        "from simple_convnet import SimpleConvNet\n",
        "from common.trainer import Trainer"
      ],
      "metadata": {
        "id": "-yuWfeuhs35j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 합성곱 계층 구현하기\n",
        "from common.util import im2col\n",
        "x1 = np.random.rand(1, 3, 7, 7).round(2)\n",
        "print(x1)\n",
        "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
        "print(col1)\n",
        "print(col1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLV-IKHZAlOr",
        "outputId": "05d9fc68-d674-4c6b-b8eb-a49c96fad5c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[0.28 0.1  0.86 0.95 0.81 0.23 0.73]\n",
            "   [0.6  0.35 0.14 0.29 0.57 0.78 0.73]\n",
            "   [0.15 0.92 0.48 0.77 0.39 0.92 0.64]\n",
            "   [0.02 0.19 0.6  0.46 0.85 0.21 0.37]\n",
            "   [0.05 0.96 0.46 0.88 0.46 0.64 0.82]\n",
            "   [0.53 0.25 1.   0.92 0.53 0.86 0.79]\n",
            "   [0.74 0.94 0.01 0.08 0.33 0.21 0.73]]\n",
            "\n",
            "  [[0.24 0.96 0.12 0.8  0.77 0.08 0.32]\n",
            "   [0.21 0.48 0.38 0.69 0.32 0.51 0.1 ]\n",
            "   [0.75 0.56 0.75 0.3  0.39 0.92 0.86]\n",
            "   [0.17 0.84 0.56 0.29 0.11 0.41 0.59]\n",
            "   [0.2  0.18 0.18 0.13 0.55 0.08 0.55]\n",
            "   [0.28 0.95 0.26 0.91 0.61 0.28 0.29]\n",
            "   [0.62 0.46 0.2  0.44 0.84 0.04 0.3 ]]\n",
            "\n",
            "  [[0.25 0.85 0.19 0.02 0.49 0.47 0.08]\n",
            "   [0.   0.62 0.89 0.18 0.87 0.55 0.44]\n",
            "   [0.07 0.52 0.55 0.4  0.28 0.31 0.86]\n",
            "   [0.66 0.14 0.65 0.49 0.89 0.04 0.64]\n",
            "   [0.83 0.95 0.39 0.8  0.55 0.13 0.83]\n",
            "   [0.66 0.15 0.92 0.53 0.1  0.45 0.52]\n",
            "   [0.77 0.13 0.85 0.83 0.6  0.85 0.46]]]]\n",
            "[[0.28 0.1  0.86 0.95 0.81 0.6  0.35 0.14 0.29 0.57 0.15 0.92 0.48 0.77\n",
            "  0.39 0.02 0.19 0.6  0.46 0.85 0.05 0.96 0.46 0.88 0.46 0.24 0.96 0.12\n",
            "  0.8  0.77 0.21 0.48 0.38 0.69 0.32 0.75 0.56 0.75 0.3  0.39 0.17 0.84\n",
            "  0.56 0.29 0.11 0.2  0.18 0.18 0.13 0.55 0.25 0.85 0.19 0.02 0.49 0.\n",
            "  0.62 0.89 0.18 0.87 0.07 0.52 0.55 0.4  0.28 0.66 0.14 0.65 0.49 0.89\n",
            "  0.83 0.95 0.39 0.8  0.55]\n",
            " [0.1  0.86 0.95 0.81 0.23 0.35 0.14 0.29 0.57 0.78 0.92 0.48 0.77 0.39\n",
            "  0.92 0.19 0.6  0.46 0.85 0.21 0.96 0.46 0.88 0.46 0.64 0.96 0.12 0.8\n",
            "  0.77 0.08 0.48 0.38 0.69 0.32 0.51 0.56 0.75 0.3  0.39 0.92 0.84 0.56\n",
            "  0.29 0.11 0.41 0.18 0.18 0.13 0.55 0.08 0.85 0.19 0.02 0.49 0.47 0.62\n",
            "  0.89 0.18 0.87 0.55 0.52 0.55 0.4  0.28 0.31 0.14 0.65 0.49 0.89 0.04\n",
            "  0.95 0.39 0.8  0.55 0.13]\n",
            " [0.86 0.95 0.81 0.23 0.73 0.14 0.29 0.57 0.78 0.73 0.48 0.77 0.39 0.92\n",
            "  0.64 0.6  0.46 0.85 0.21 0.37 0.46 0.88 0.46 0.64 0.82 0.12 0.8  0.77\n",
            "  0.08 0.32 0.38 0.69 0.32 0.51 0.1  0.75 0.3  0.39 0.92 0.86 0.56 0.29\n",
            "  0.11 0.41 0.59 0.18 0.13 0.55 0.08 0.55 0.19 0.02 0.49 0.47 0.08 0.89\n",
            "  0.18 0.87 0.55 0.44 0.55 0.4  0.28 0.31 0.86 0.65 0.49 0.89 0.04 0.64\n",
            "  0.39 0.8  0.55 0.13 0.83]\n",
            " [0.6  0.35 0.14 0.29 0.57 0.15 0.92 0.48 0.77 0.39 0.02 0.19 0.6  0.46\n",
            "  0.85 0.05 0.96 0.46 0.88 0.46 0.53 0.25 1.   0.92 0.53 0.21 0.48 0.38\n",
            "  0.69 0.32 0.75 0.56 0.75 0.3  0.39 0.17 0.84 0.56 0.29 0.11 0.2  0.18\n",
            "  0.18 0.13 0.55 0.28 0.95 0.26 0.91 0.61 0.   0.62 0.89 0.18 0.87 0.07\n",
            "  0.52 0.55 0.4  0.28 0.66 0.14 0.65 0.49 0.89 0.83 0.95 0.39 0.8  0.55\n",
            "  0.66 0.15 0.92 0.53 0.1 ]\n",
            " [0.35 0.14 0.29 0.57 0.78 0.92 0.48 0.77 0.39 0.92 0.19 0.6  0.46 0.85\n",
            "  0.21 0.96 0.46 0.88 0.46 0.64 0.25 1.   0.92 0.53 0.86 0.48 0.38 0.69\n",
            "  0.32 0.51 0.56 0.75 0.3  0.39 0.92 0.84 0.56 0.29 0.11 0.41 0.18 0.18\n",
            "  0.13 0.55 0.08 0.95 0.26 0.91 0.61 0.28 0.62 0.89 0.18 0.87 0.55 0.52\n",
            "  0.55 0.4  0.28 0.31 0.14 0.65 0.49 0.89 0.04 0.95 0.39 0.8  0.55 0.13\n",
            "  0.15 0.92 0.53 0.1  0.45]\n",
            " [0.14 0.29 0.57 0.78 0.73 0.48 0.77 0.39 0.92 0.64 0.6  0.46 0.85 0.21\n",
            "  0.37 0.46 0.88 0.46 0.64 0.82 1.   0.92 0.53 0.86 0.79 0.38 0.69 0.32\n",
            "  0.51 0.1  0.75 0.3  0.39 0.92 0.86 0.56 0.29 0.11 0.41 0.59 0.18 0.13\n",
            "  0.55 0.08 0.55 0.26 0.91 0.61 0.28 0.29 0.89 0.18 0.87 0.55 0.44 0.55\n",
            "  0.4  0.28 0.31 0.86 0.65 0.49 0.89 0.04 0.64 0.39 0.8  0.55 0.13 0.83\n",
            "  0.92 0.53 0.1  0.45 0.52]\n",
            " [0.15 0.92 0.48 0.77 0.39 0.02 0.19 0.6  0.46 0.85 0.05 0.96 0.46 0.88\n",
            "  0.46 0.53 0.25 1.   0.92 0.53 0.74 0.94 0.01 0.08 0.33 0.75 0.56 0.75\n",
            "  0.3  0.39 0.17 0.84 0.56 0.29 0.11 0.2  0.18 0.18 0.13 0.55 0.28 0.95\n",
            "  0.26 0.91 0.61 0.62 0.46 0.2  0.44 0.84 0.07 0.52 0.55 0.4  0.28 0.66\n",
            "  0.14 0.65 0.49 0.89 0.83 0.95 0.39 0.8  0.55 0.66 0.15 0.92 0.53 0.1\n",
            "  0.77 0.13 0.85 0.83 0.6 ]\n",
            " [0.92 0.48 0.77 0.39 0.92 0.19 0.6  0.46 0.85 0.21 0.96 0.46 0.88 0.46\n",
            "  0.64 0.25 1.   0.92 0.53 0.86 0.94 0.01 0.08 0.33 0.21 0.56 0.75 0.3\n",
            "  0.39 0.92 0.84 0.56 0.29 0.11 0.41 0.18 0.18 0.13 0.55 0.08 0.95 0.26\n",
            "  0.91 0.61 0.28 0.46 0.2  0.44 0.84 0.04 0.52 0.55 0.4  0.28 0.31 0.14\n",
            "  0.65 0.49 0.89 0.04 0.95 0.39 0.8  0.55 0.13 0.15 0.92 0.53 0.1  0.45\n",
            "  0.13 0.85 0.83 0.6  0.85]\n",
            " [0.48 0.77 0.39 0.92 0.64 0.6  0.46 0.85 0.21 0.37 0.46 0.88 0.46 0.64\n",
            "  0.82 1.   0.92 0.53 0.86 0.79 0.01 0.08 0.33 0.21 0.73 0.75 0.3  0.39\n",
            "  0.92 0.86 0.56 0.29 0.11 0.41 0.59 0.18 0.13 0.55 0.08 0.55 0.26 0.91\n",
            "  0.61 0.28 0.29 0.2  0.44 0.84 0.04 0.3  0.55 0.4  0.28 0.31 0.86 0.65\n",
            "  0.49 0.89 0.04 0.64 0.39 0.8  0.55 0.13 0.83 0.92 0.53 0.1  0.45 0.52\n",
            "  0.85 0.83 0.6  0.85 0.46]]\n",
            "(9, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = np.random.rand(10, 3, 7, 7).round(2)\n",
        "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
        "print(col2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV5NiI5rFZDK",
        "outputId": "801e358b-9d9f-4fc5-cc9f-f4fe21e47caa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(90, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleConvNet:\n",
        "    \"\"\"단순한 합성곱 신경망\n",
        "    \n",
        "    conv - relu - pool - affine - relu - affine - softmax\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
        "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
        "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
        "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
        "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
        "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
        "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(1, 28, 28),  # 784\n",
        "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "        # 가중치 매개변수 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "                            np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        self.params['W3'] = weight_init_std * \\\n",
        "                            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()   # 순서가 있는 딕셔너리\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                           conv_param['stride'], conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \"\"\"손실 함수를 구한다.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        \"\"\"\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        \n",
        "        acc = 0.0\n",
        "        \n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt) \n",
        "        \n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다（수치미분）.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        loss_w = lambda w: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in (1, 2, 3):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다(오차역전파법).\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "        \n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
        "            self.layers[key].W = self.params['W' + str(i+1)]\n",
        "            self.layers[key].b = self.params['b' + str(i+1)]\n"
      ],
      "metadata": {
        "id": "AT1SDylktkze"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
        "x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28), \n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "                        \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# 매개변수 보존\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "07-RaFBZ84Cb",
        "outputId": "737839c7-d642-46b8-b299-fda289fa212f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss:2.3004467198782708\n",
            "=== epoch:1, train acc:0.212, test acc:0.22 ===\n",
            "train loss:2.2979979378763042\n",
            "train loss:2.294436635135244\n",
            "train loss:2.2889386669249223\n",
            "train loss:2.278878215943522\n",
            "train loss:2.2649665692519325\n",
            "train loss:2.251646987507864\n",
            "train loss:2.243584515498566\n",
            "train loss:2.2184291359106094\n",
            "train loss:2.1691368743387556\n",
            "train loss:2.1728396514295545\n",
            "train loss:2.1233183447042294\n",
            "train loss:2.1117026799128062\n",
            "train loss:2.083595183462589\n",
            "train loss:2.024002374545422\n",
            "train loss:1.9406015959597003\n",
            "train loss:1.943162483284416\n",
            "train loss:1.7849305760091132\n",
            "train loss:1.7085430609734595\n",
            "train loss:1.6756257877477845\n",
            "train loss:1.6387422158815874\n",
            "train loss:1.5577470426402646\n",
            "train loss:1.4179124053445158\n",
            "train loss:1.3953070262789118\n",
            "train loss:1.3066014612830414\n",
            "train loss:1.1484291063716814\n",
            "train loss:1.0618136477293776\n",
            "train loss:1.0230544843133864\n",
            "train loss:0.9422544282579434\n",
            "train loss:1.1007822771309914\n",
            "train loss:0.8709967496800517\n",
            "train loss:0.9418830343442502\n",
            "train loss:0.809821109882289\n",
            "train loss:0.7057736919543989\n",
            "train loss:0.748281056714458\n",
            "train loss:0.6840072067984156\n",
            "train loss:0.6231324217884158\n",
            "train loss:0.6369631295811805\n",
            "train loss:0.6363421069244476\n",
            "train loss:0.5944496719782831\n",
            "train loss:0.573396271449698\n",
            "train loss:0.7603206167278954\n",
            "train loss:0.5824278480945076\n",
            "train loss:0.6091306979882014\n",
            "train loss:0.6492530459325416\n",
            "train loss:0.4468946171383877\n",
            "train loss:0.6564328212491514\n",
            "train loss:0.43494131602940894\n",
            "train loss:0.4832418166482191\n",
            "train loss:0.5518673999556294\n",
            "train loss:0.5121125985650375\n",
            "=== epoch:2, train acc:0.826, test acc:0.802 ===\n",
            "train loss:0.5700200411199718\n",
            "train loss:0.6037932516376544\n",
            "train loss:0.5468331257916001\n",
            "train loss:0.6167649200678315\n",
            "train loss:0.4030095956488027\n",
            "train loss:0.6328957460294664\n",
            "train loss:0.38193908833782053\n",
            "train loss:0.38276243057391407\n",
            "train loss:0.4803768099419329\n",
            "train loss:0.43949375790556056\n",
            "train loss:0.30763993554453933\n",
            "train loss:0.4705285313858292\n",
            "train loss:0.41215473165484573\n",
            "train loss:0.33591540115288154\n",
            "train loss:0.5521678128728089\n",
            "train loss:0.36545414158606476\n",
            "train loss:0.4147481236226851\n",
            "train loss:0.4117146614158828\n",
            "train loss:0.2674499035279304\n",
            "train loss:0.34048170506274217\n",
            "train loss:0.23502565910335008\n",
            "train loss:0.45948361159676054\n",
            "train loss:0.3841566360393784\n",
            "train loss:0.4700687505151791\n",
            "train loss:0.5776895079062642\n",
            "train loss:0.5907275987763814\n",
            "train loss:0.2863255655198964\n",
            "train loss:0.3210719936929813\n",
            "train loss:0.4717795528463868\n",
            "train loss:0.2903394743010048\n",
            "train loss:0.27822623446956385\n",
            "train loss:0.4543427257090285\n",
            "train loss:0.4728062947581141\n",
            "train loss:0.49124652549014397\n",
            "train loss:0.2851505580681929\n",
            "train loss:0.23437456748324262\n",
            "train loss:0.2913605508372026\n",
            "train loss:0.5781588657501979\n",
            "train loss:0.4690608140894865\n",
            "train loss:0.32086151977805394\n",
            "train loss:0.3550965797889123\n",
            "train loss:0.3921323257570997\n",
            "train loss:0.369011497813908\n",
            "train loss:0.2488831111129084\n",
            "train loss:0.32921454936688554\n",
            "train loss:0.506729785813329\n",
            "train loss:0.5726715844642035\n",
            "train loss:0.22528068640595286\n",
            "train loss:0.26486874845997804\n",
            "train loss:0.377441386305563\n",
            "=== epoch:3, train acc:0.891, test acc:0.872 ===\n",
            "train loss:0.29688198180291087\n",
            "train loss:0.30163179744259394\n",
            "train loss:0.4464913033270301\n",
            "train loss:0.2687677750206192\n",
            "train loss:0.3584696667548368\n",
            "train loss:0.3024381757773103\n",
            "train loss:0.39884702258210747\n",
            "train loss:0.3436313329307745\n",
            "train loss:0.3555290310072145\n",
            "train loss:0.25343736991523336\n",
            "train loss:0.2276734880567932\n",
            "train loss:0.5758520872210524\n",
            "train loss:0.48123486791170644\n",
            "train loss:0.2218336988007805\n",
            "train loss:0.2503710359067086\n",
            "train loss:0.27699510186001364\n",
            "train loss:0.41846581156902635\n",
            "train loss:0.33588384383667913\n",
            "train loss:0.2510990156949983\n",
            "train loss:0.3204363088890827\n",
            "train loss:0.18006562276950447\n",
            "train loss:0.26407520112116317\n",
            "train loss:0.2048407202842131\n",
            "train loss:0.2473594165394185\n",
            "train loss:0.2565270130390768\n",
            "train loss:0.25594504753751446\n",
            "train loss:0.34724267359436456\n",
            "train loss:0.3983070798376177\n",
            "train loss:0.24181878332497825\n",
            "train loss:0.2694036228595019\n",
            "train loss:0.3382266597346698\n",
            "train loss:0.47002924645127026\n",
            "train loss:0.23860739315918653\n",
            "train loss:0.39233320904348573\n",
            "train loss:0.28786259407820025\n",
            "train loss:0.36359998239088903\n",
            "train loss:0.27981051568823795\n",
            "train loss:0.4773410058211084\n",
            "train loss:0.26029857340107176\n",
            "train loss:0.38529500065718536\n",
            "train loss:0.2515420305195592\n",
            "train loss:0.2137032817743942\n",
            "train loss:0.2974942175542434\n",
            "train loss:0.22560493128598938\n",
            "train loss:0.23945843829274374\n",
            "train loss:0.2453659720319455\n",
            "train loss:0.2971030717126466\n",
            "train loss:0.31277545095417114\n",
            "train loss:0.33836157099573255\n",
            "train loss:0.28306783557815657\n",
            "=== epoch:4, train acc:0.895, test acc:0.89 ===\n",
            "train loss:0.25875540237935984\n",
            "train loss:0.3500349253029591\n",
            "train loss:0.31162945130210695\n",
            "train loss:0.38950223047028826\n",
            "train loss:0.2051790897787166\n",
            "train loss:0.37296758135107716\n",
            "train loss:0.3213349365380987\n",
            "train loss:0.17630068395564658\n",
            "train loss:0.3701925301335703\n",
            "train loss:0.2141761531473044\n",
            "train loss:0.3022628491838661\n",
            "train loss:0.2701724069947576\n",
            "train loss:0.24597060394997924\n",
            "train loss:0.29898175460906623\n",
            "train loss:0.25839283225905435\n",
            "train loss:0.2705515171930098\n",
            "train loss:0.22344751466863083\n",
            "train loss:0.2589643494488066\n",
            "train loss:0.24800476854967698\n",
            "train loss:0.1897308838415855\n",
            "train loss:0.21158366452878272\n",
            "train loss:0.3063788888119725\n",
            "train loss:0.14485254309191592\n",
            "train loss:0.3874938043192377\n",
            "train loss:0.3183566710565031\n",
            "train loss:0.24804802649359142\n",
            "train loss:0.2489448799544457\n",
            "train loss:0.23797863332203734\n",
            "train loss:0.2984975376067273\n",
            "train loss:0.15982987342244084\n",
            "train loss:0.343723919623697\n",
            "train loss:0.13452156721190636\n",
            "train loss:0.15833823620502696\n",
            "train loss:0.3758515587954252\n",
            "train loss:0.26568622356892635\n",
            "train loss:0.3639747447804292\n",
            "train loss:0.23814007944221047\n",
            "train loss:0.25528461064449737\n",
            "train loss:0.20644445878801243\n",
            "train loss:0.5836381771366314\n",
            "train loss:0.2609103756590608\n",
            "train loss:0.1994853638794374\n",
            "train loss:0.22430023804795962\n",
            "train loss:0.2912675700731793\n",
            "train loss:0.222552756584929\n",
            "train loss:0.20853667079702343\n",
            "train loss:0.13259251865843238\n",
            "train loss:0.23050575212480012\n",
            "train loss:0.3920665003904938\n",
            "train loss:0.20153958547817644\n",
            "=== epoch:5, train acc:0.912, test acc:0.906 ===\n",
            "train loss:0.2243540864180903\n",
            "train loss:0.28565324408361437\n",
            "train loss:0.24675504747879162\n",
            "train loss:0.36790810062107077\n",
            "train loss:0.1839867564865847\n",
            "train loss:0.21512664269295897\n",
            "train loss:0.20286136298561883\n",
            "train loss:0.2716906810209161\n",
            "train loss:0.20924937332009175\n",
            "train loss:0.20938618464629258\n",
            "train loss:0.2408554553286986\n",
            "train loss:0.18305961501121945\n",
            "train loss:0.420982071029198\n",
            "train loss:0.28358343685031956\n",
            "train loss:0.20689775101612906\n",
            "train loss:0.17878065793480288\n",
            "train loss:0.1801480296437098\n",
            "train loss:0.2895409638467797\n",
            "train loss:0.2036244370999259\n",
            "train loss:0.18812634981502177\n",
            "train loss:0.3345187923786703\n",
            "train loss:0.16048774149157383\n",
            "train loss:0.12050024757809162\n",
            "train loss:0.2516657204851932\n",
            "train loss:0.12383420167250984\n",
            "train loss:0.19233888545193328\n",
            "train loss:0.23983457297200567\n",
            "train loss:0.1372752794557518\n",
            "train loss:0.21680778978922077\n",
            "train loss:0.15418292324524513\n",
            "train loss:0.2846975781383172\n",
            "train loss:0.2822495179646201\n",
            "train loss:0.2389084212851541\n",
            "train loss:0.16704412430629995\n",
            "train loss:0.30434878560869033\n",
            "train loss:0.3061567801907964\n",
            "train loss:0.3196458833654577\n",
            "train loss:0.1565407883973711\n",
            "train loss:0.3775276372992943\n",
            "train loss:0.31169503732103093\n",
            "train loss:0.2587316306829771\n",
            "train loss:0.20643042238758466\n",
            "train loss:0.21012068297642114\n",
            "train loss:0.13345107083732205\n",
            "train loss:0.20606074990470305\n",
            "train loss:0.19935272376986993\n",
            "train loss:0.29471862513702257\n",
            "train loss:0.16059591852774005\n",
            "train loss:0.144291495775155\n",
            "train loss:0.19661001331499187\n",
            "=== epoch:6, train acc:0.93, test acc:0.916 ===\n",
            "train loss:0.20091162790396286\n",
            "train loss:0.14324843914369698\n",
            "train loss:0.1552658164747774\n",
            "train loss:0.19499466832996323\n",
            "train loss:0.2862047548151252\n",
            "train loss:0.1727992380473393\n",
            "train loss:0.15901269212730307\n",
            "train loss:0.09560896432885484\n",
            "train loss:0.22142120617376906\n",
            "train loss:0.20099231382395966\n",
            "train loss:0.20878980910556535\n",
            "train loss:0.18964288339145888\n",
            "train loss:0.31264984232542786\n",
            "train loss:0.10782093898196134\n",
            "train loss:0.2773431706724804\n",
            "train loss:0.165172760589328\n",
            "train loss:0.12148681451906367\n",
            "train loss:0.2233399683560476\n",
            "train loss:0.10754759162196693\n",
            "train loss:0.28486414519771\n",
            "train loss:0.20407093702255288\n",
            "train loss:0.13945425716681756\n",
            "train loss:0.13669784750319053\n",
            "train loss:0.18810483694686517\n",
            "train loss:0.14823840582975117\n",
            "train loss:0.2019233764673934\n",
            "train loss:0.2046003209922678\n",
            "train loss:0.15386541731845355\n",
            "train loss:0.16218106729237142\n",
            "train loss:0.26211732254796183\n",
            "train loss:0.1434197304467957\n",
            "train loss:0.08620022466113739\n",
            "train loss:0.11645331217289318\n",
            "train loss:0.17234866954064185\n",
            "train loss:0.12268995041888302\n",
            "train loss:0.208431245211918\n",
            "train loss:0.15225394331574718\n",
            "train loss:0.21108891600804966\n",
            "train loss:0.11538747613646269\n",
            "train loss:0.26074885268486797\n",
            "train loss:0.1622435230987931\n",
            "train loss:0.09506107532884542\n",
            "train loss:0.16942288452777002\n",
            "train loss:0.11257123246226108\n",
            "train loss:0.12120105856460539\n",
            "train loss:0.15348385215962737\n",
            "train loss:0.19867467048913284\n",
            "train loss:0.1373514685637461\n",
            "train loss:0.1456682193325161\n",
            "train loss:0.22325886667281789\n",
            "=== epoch:7, train acc:0.94, test acc:0.917 ===\n",
            "train loss:0.25373102477199977\n",
            "train loss:0.10269490219527377\n",
            "train loss:0.0626493954447333\n",
            "train loss:0.17201054508956104\n",
            "train loss:0.10053129985543478\n",
            "train loss:0.14298888660440762\n",
            "train loss:0.17856990153718125\n",
            "train loss:0.18820003256148632\n",
            "train loss:0.22547554655370305\n",
            "train loss:0.20138634799015423\n",
            "train loss:0.10986132304526246\n",
            "train loss:0.15506295823838762\n",
            "train loss:0.19660549774207792\n",
            "train loss:0.19283702594016527\n",
            "train loss:0.11393879012498187\n",
            "train loss:0.1549103427091306\n",
            "train loss:0.24943407357970657\n",
            "train loss:0.17403211129676202\n",
            "train loss:0.07353634328078029\n",
            "train loss:0.16076016705698493\n",
            "train loss:0.05563902246434156\n",
            "train loss:0.07275147981589722\n",
            "train loss:0.07962378921054976\n",
            "train loss:0.16323744204655874\n",
            "train loss:0.1366433951945971\n",
            "train loss:0.23201528787264286\n",
            "train loss:0.05508881303556809\n",
            "train loss:0.237075297718322\n",
            "train loss:0.12029696249219621\n",
            "train loss:0.17137587953173042\n",
            "train loss:0.19986343164212056\n",
            "train loss:0.11899500024311145\n",
            "train loss:0.07566103569095327\n",
            "train loss:0.21640054581477444\n",
            "train loss:0.10121083176968348\n",
            "train loss:0.19439575219095315\n",
            "train loss:0.1485461442034362\n",
            "train loss:0.07964954283782037\n",
            "train loss:0.11544873259248847\n",
            "train loss:0.1603108137356197\n",
            "train loss:0.1566137569809135\n",
            "train loss:0.07277932242800238\n",
            "train loss:0.1393299120090474\n",
            "train loss:0.16724638134752748\n",
            "train loss:0.1279135524848376\n",
            "train loss:0.09111384541919249\n",
            "train loss:0.11361215944320126\n",
            "train loss:0.16326536706198663\n",
            "train loss:0.17166634437122327\n",
            "train loss:0.10885851331928248\n",
            "=== epoch:8, train acc:0.944, test acc:0.931 ===\n",
            "train loss:0.1806237082892368\n",
            "train loss:0.0939466901842366\n",
            "train loss:0.1881425583816169\n",
            "train loss:0.17036442399462776\n",
            "train loss:0.091094346658549\n",
            "train loss:0.11833850949324712\n",
            "train loss:0.22450610855309314\n",
            "train loss:0.1315182692011926\n",
            "train loss:0.1266881074356533\n",
            "train loss:0.1287857385978285\n",
            "train loss:0.10396475293100922\n",
            "train loss:0.06340681160770396\n",
            "train loss:0.18935306912386168\n",
            "train loss:0.10925497113640997\n",
            "train loss:0.19865474072938605\n",
            "train loss:0.08775615975694048\n",
            "train loss:0.13023801471235064\n",
            "train loss:0.08178785971257733\n",
            "train loss:0.1757220541608792\n",
            "train loss:0.16980720094260696\n",
            "train loss:0.13235938493588464\n",
            "train loss:0.14382546933824192\n",
            "train loss:0.12607270221674885\n",
            "train loss:0.18063615866057323\n",
            "train loss:0.16960414612409566\n",
            "train loss:0.08768568396424348\n",
            "train loss:0.11237991359653916\n",
            "train loss:0.10947751232639177\n",
            "train loss:0.07047199213442662\n",
            "train loss:0.05776486860173839\n",
            "train loss:0.1087882160466682\n",
            "train loss:0.08352841885614\n",
            "train loss:0.11285061792829248\n",
            "train loss:0.08441536383716537\n",
            "train loss:0.1075205518453299\n",
            "train loss:0.1260672879425917\n",
            "train loss:0.06956803091284927\n",
            "train loss:0.08200499638546484\n",
            "train loss:0.11038242121282942\n",
            "train loss:0.11614026210301853\n",
            "train loss:0.14766617269039317\n",
            "train loss:0.15020590656279867\n",
            "train loss:0.1253612410142948\n",
            "train loss:0.11780526260319991\n",
            "train loss:0.13003377908386907\n",
            "train loss:0.05044608907746308\n",
            "train loss:0.1478727753905787\n",
            "train loss:0.13929857177153881\n",
            "train loss:0.07750601558688838\n",
            "train loss:0.058499888536889894\n",
            "=== epoch:9, train acc:0.954, test acc:0.934 ===\n",
            "train loss:0.17623841226945625\n",
            "train loss:0.19147950035433367\n",
            "train loss:0.07085018461577529\n",
            "train loss:0.06146505538147588\n",
            "train loss:0.14767079503058203\n",
            "train loss:0.18340834798912742\n",
            "train loss:0.0990611117719448\n",
            "train loss:0.08042767494708046\n",
            "train loss:0.1215145282641144\n",
            "train loss:0.08875970323484023\n",
            "train loss:0.16918559767126276\n",
            "train loss:0.1670402577544156\n",
            "train loss:0.19946848487906185\n",
            "train loss:0.07903133699465413\n",
            "train loss:0.1333966436678204\n",
            "train loss:0.08785341366521525\n",
            "train loss:0.13492857360691027\n",
            "train loss:0.13192090161990827\n",
            "train loss:0.19336574125999303\n",
            "train loss:0.14081532515341846\n",
            "train loss:0.059577365025468454\n",
            "train loss:0.08534507928177462\n",
            "train loss:0.07245186647797057\n",
            "train loss:0.07147228401848699\n",
            "train loss:0.11490692642082585\n",
            "train loss:0.1426740553755318\n",
            "train loss:0.12113151980477024\n",
            "train loss:0.0687730667769205\n",
            "train loss:0.06170350816502873\n",
            "train loss:0.05892386201651866\n",
            "train loss:0.09079412775683837\n",
            "train loss:0.07479441301228178\n",
            "train loss:0.07893059333236781\n",
            "train loss:0.1182901891223518\n",
            "train loss:0.09157088757923207\n",
            "train loss:0.15709487116595303\n",
            "train loss:0.07193888313455667\n",
            "train loss:0.14240660719974818\n",
            "train loss:0.1592705461218121\n",
            "train loss:0.06717754240273129\n",
            "train loss:0.06409261848754902\n",
            "train loss:0.10502828913635104\n",
            "train loss:0.18912068316303196\n",
            "train loss:0.06963326764987134\n",
            "train loss:0.18866260997574386\n",
            "train loss:0.131853820630372\n",
            "train loss:0.07196713735142402\n",
            "train loss:0.09525214087914823\n",
            "train loss:0.060361413624064116\n",
            "train loss:0.08017379970393572\n",
            "=== epoch:10, train acc:0.969, test acc:0.942 ===\n",
            "train loss:0.12588957853130028\n",
            "train loss:0.06652207389094755\n",
            "train loss:0.06202158681550611\n",
            "train loss:0.14112798198761728\n",
            "train loss:0.13414635234057815\n",
            "train loss:0.1991326454513322\n",
            "train loss:0.0544500642684134\n",
            "train loss:0.05031019487243991\n",
            "train loss:0.12466274678276956\n",
            "train loss:0.1498565770233227\n",
            "train loss:0.15647884365039252\n",
            "train loss:0.10726129857601686\n",
            "train loss:0.057678573891565124\n",
            "train loss:0.042304586497769386\n",
            "train loss:0.08075290909328832\n",
            "train loss:0.08775108700998902\n",
            "train loss:0.07888797848434159\n",
            "train loss:0.13998746122965922\n",
            "train loss:0.05906819307862848\n",
            "train loss:0.10159180741452933\n",
            "train loss:0.07728357946443737\n",
            "train loss:0.042162460780632814\n",
            "train loss:0.07691859864718226\n",
            "train loss:0.02825857995088456\n",
            "train loss:0.13923630451795113\n",
            "train loss:0.07329045031692896\n",
            "train loss:0.10362682681927754\n",
            "train loss:0.09145971366624965\n",
            "train loss:0.12985038526330064\n",
            "train loss:0.06262800667757286\n",
            "train loss:0.11463520432291947\n",
            "train loss:0.04557214774580928\n",
            "train loss:0.07118812875961866\n",
            "train loss:0.09416937479975931\n",
            "train loss:0.08181661474736296\n",
            "train loss:0.10583656850357073\n",
            "train loss:0.0743752805830843\n",
            "train loss:0.04968944329618922\n",
            "train loss:0.04489382986601616\n",
            "train loss:0.08122007402240068\n",
            "train loss:0.10888565222794488\n",
            "train loss:0.05069356243892443\n",
            "train loss:0.08711299256224139\n",
            "train loss:0.0642168510106941\n",
            "train loss:0.09466563196466085\n",
            "train loss:0.07853451528730114\n",
            "train loss:0.07623120653914965\n",
            "train loss:0.04876320836808641\n",
            "train loss:0.09869471579422945\n",
            "train loss:0.1362729075613618\n",
            "=== epoch:11, train acc:0.972, test acc:0.948 ===\n",
            "train loss:0.09632052176311494\n",
            "train loss:0.0659102746571994\n",
            "train loss:0.0774879226944393\n",
            "train loss:0.10815945119451868\n",
            "train loss:0.045409391076592094\n",
            "train loss:0.0726365093740748\n",
            "train loss:0.08966950769329729\n",
            "train loss:0.11487485666742935\n",
            "train loss:0.04890250398391422\n",
            "train loss:0.07961482832088025\n",
            "train loss:0.07780913507877484\n",
            "train loss:0.07604704432725416\n",
            "train loss:0.057186386743006495\n",
            "train loss:0.049382149882880776\n",
            "train loss:0.1494638440919305\n",
            "train loss:0.05269813848366816\n",
            "train loss:0.10228137985481055\n",
            "train loss:0.030764034909278225\n",
            "train loss:0.14194586527296477\n",
            "train loss:0.13858360534519115\n",
            "train loss:0.0406184481761524\n",
            "train loss:0.10686831857094224\n",
            "train loss:0.08267765441910285\n",
            "train loss:0.05929076116970431\n",
            "train loss:0.0948861745014468\n",
            "train loss:0.062075511490027796\n",
            "train loss:0.07757583758121386\n",
            "train loss:0.08649427682413795\n",
            "train loss:0.06032063831924563\n",
            "train loss:0.05100423329552951\n",
            "train loss:0.04065520970167115\n",
            "train loss:0.08045698027346683\n",
            "train loss:0.05233411586034889\n",
            "train loss:0.22297286272493824\n",
            "train loss:0.03309163983991156\n",
            "train loss:0.06055583606520905\n",
            "train loss:0.03761823486860247\n",
            "train loss:0.08884137280631527\n",
            "train loss:0.04446916569331377\n",
            "train loss:0.03553533867680832\n",
            "train loss:0.03116368343466451\n",
            "train loss:0.09176796016560268\n",
            "train loss:0.04329477963178786\n",
            "train loss:0.03580391264863097\n",
            "train loss:0.08955437534287893\n",
            "train loss:0.07898390307112763\n",
            "train loss:0.07547267175468933\n",
            "train loss:0.06364229276861316\n",
            "train loss:0.05944661789820346\n",
            "train loss:0.10501638415775463\n",
            "=== epoch:12, train acc:0.972, test acc:0.947 ===\n",
            "train loss:0.050943210680189394\n",
            "train loss:0.05702629795148005\n",
            "train loss:0.09204241495657327\n",
            "train loss:0.1129918604363255\n",
            "train loss:0.06255005221625601\n",
            "train loss:0.07860844341759911\n",
            "train loss:0.060085884007971775\n",
            "train loss:0.04154755082020806\n",
            "train loss:0.03248108363840161\n",
            "train loss:0.060507336681024926\n",
            "train loss:0.04599329246667945\n",
            "train loss:0.05282185317560631\n",
            "train loss:0.08458057105200824\n",
            "train loss:0.06909848754607983\n",
            "train loss:0.08817897078961554\n",
            "train loss:0.10878301639872275\n",
            "train loss:0.027774861970920463\n",
            "train loss:0.10952942224064721\n",
            "train loss:0.06554860222239847\n",
            "train loss:0.09308546492796807\n",
            "train loss:0.1060560077395655\n",
            "train loss:0.11388871517178568\n",
            "train loss:0.06485151596682984\n",
            "train loss:0.09365029210053624\n",
            "train loss:0.03188022352179684\n",
            "train loss:0.01993988115500723\n",
            "train loss:0.039072908426712384\n",
            "train loss:0.11608699099248758\n",
            "train loss:0.049963908254220216\n",
            "train loss:0.10869837715142569\n",
            "train loss:0.05769758779644066\n",
            "train loss:0.05901261425760257\n",
            "train loss:0.06168396952876101\n",
            "train loss:0.036466534072877664\n",
            "train loss:0.11245845818601556\n",
            "train loss:0.05738921630794676\n",
            "train loss:0.0470338547426594\n",
            "train loss:0.0154790385812663\n",
            "train loss:0.015384958482660846\n",
            "train loss:0.08623989081779403\n",
            "train loss:0.09494518134019443\n",
            "train loss:0.0391656904595301\n",
            "train loss:0.08062624601518956\n",
            "train loss:0.11062293394666421\n",
            "train loss:0.02904796727595807\n",
            "train loss:0.054792557568127655\n",
            "train loss:0.05908695865422313\n",
            "train loss:0.11244446303761904\n",
            "train loss:0.04821544304022784\n",
            "train loss:0.05853840675864869\n",
            "=== epoch:13, train acc:0.98, test acc:0.955 ===\n",
            "train loss:0.0499201825038377\n",
            "train loss:0.03196336412393168\n",
            "train loss:0.09112994289932663\n",
            "train loss:0.05570953398980673\n",
            "train loss:0.07539958707653044\n",
            "train loss:0.023829323657315465\n",
            "train loss:0.05780464814390271\n",
            "train loss:0.034831000717033\n",
            "train loss:0.12220395135195983\n",
            "train loss:0.05064813158306829\n",
            "train loss:0.022982141033907433\n",
            "train loss:0.03231539158084527\n",
            "train loss:0.08211583161746154\n",
            "train loss:0.02862798724150081\n",
            "train loss:0.06062869630920819\n",
            "train loss:0.10664727922264128\n",
            "train loss:0.15680436662293767\n",
            "train loss:0.03357609703550163\n",
            "train loss:0.033391778160635016\n",
            "train loss:0.03704220440390782\n",
            "train loss:0.023102076523929812\n",
            "train loss:0.04711170349929089\n",
            "train loss:0.03938844018569445\n",
            "train loss:0.06486464865585241\n",
            "train loss:0.05210419612015512\n",
            "train loss:0.06366747761940153\n",
            "train loss:0.030230983909956363\n",
            "train loss:0.046240716415318545\n",
            "train loss:0.024349282154321555\n",
            "train loss:0.043412373256930946\n",
            "train loss:0.06978889556337181\n",
            "train loss:0.05650840980227442\n",
            "train loss:0.06575165963464036\n",
            "train loss:0.09612738647748996\n",
            "train loss:0.05313849739680939\n",
            "train loss:0.06121034797866761\n",
            "train loss:0.07806739847533162\n",
            "train loss:0.031521032330402744\n",
            "train loss:0.06021324458661015\n",
            "train loss:0.035791750030096374\n",
            "train loss:0.022549776573125384\n",
            "train loss:0.04834941569445622\n",
            "train loss:0.03414777763395341\n",
            "train loss:0.04488472261921207\n",
            "train loss:0.028912153923239457\n",
            "train loss:0.10445909865299426\n",
            "train loss:0.02961468801690112\n",
            "train loss:0.04153322411574961\n",
            "train loss:0.04566194540635243\n",
            "train loss:0.054761054876001115\n",
            "=== epoch:14, train acc:0.983, test acc:0.955 ===\n",
            "train loss:0.07311433643982809\n",
            "train loss:0.01833658123395383\n",
            "train loss:0.07632723719658198\n",
            "train loss:0.06954990606179444\n",
            "train loss:0.1124451508898807\n",
            "train loss:0.027990117109369973\n",
            "train loss:0.043404693967783674\n",
            "train loss:0.04167348670511495\n",
            "train loss:0.0869084120554282\n",
            "train loss:0.07932652398219787\n",
            "train loss:0.02500133905291975\n",
            "train loss:0.08425272947197147\n",
            "train loss:0.034723319648693816\n",
            "train loss:0.04192786469184769\n",
            "train loss:0.06024898729150203\n",
            "train loss:0.05113316345259915\n",
            "train loss:0.04809833378628523\n",
            "train loss:0.021087419970861797\n",
            "train loss:0.05030311512588349\n",
            "train loss:0.05640267709343547\n",
            "train loss:0.07885718457757378\n",
            "train loss:0.05528594527895369\n",
            "train loss:0.025674978186043296\n",
            "train loss:0.06194472038844191\n",
            "train loss:0.04160291061832872\n",
            "train loss:0.041472909287804664\n",
            "train loss:0.06794128501087986\n",
            "train loss:0.04843596207447157\n",
            "train loss:0.07422328135590303\n",
            "train loss:0.048043728257077624\n",
            "train loss:0.048101112306971616\n",
            "train loss:0.028481597784034564\n",
            "train loss:0.0857594673427523\n",
            "train loss:0.044683567530096495\n",
            "train loss:0.035292531518496754\n",
            "train loss:0.030949479044730846\n",
            "train loss:0.015391619947692661\n",
            "train loss:0.07532151606540158\n",
            "train loss:0.03507928107660249\n",
            "train loss:0.054218847404673774\n",
            "train loss:0.02779940887545784\n",
            "train loss:0.04706035272251033\n",
            "train loss:0.025681058491359952\n",
            "train loss:0.04283498277373145\n",
            "train loss:0.04684907519218448\n",
            "train loss:0.03150045576913059\n",
            "train loss:0.015360339020060516\n",
            "train loss:0.026699362575618574\n",
            "train loss:0.0376274326036333\n",
            "train loss:0.02531143802387464\n",
            "=== epoch:15, train acc:0.983, test acc:0.956 ===\n",
            "train loss:0.06761788713006076\n",
            "train loss:0.02660351544691048\n",
            "train loss:0.02638316928359409\n",
            "train loss:0.0411508507876459\n",
            "train loss:0.050706421918610405\n",
            "train loss:0.06874265775275305\n",
            "train loss:0.04354564740898271\n",
            "train loss:0.03980874868233411\n",
            "train loss:0.054228767469026246\n",
            "train loss:0.010191552420606544\n",
            "train loss:0.04477876182826718\n",
            "train loss:0.04006551074085029\n",
            "train loss:0.05916190226250364\n",
            "train loss:0.031748395551635944\n",
            "train loss:0.03895894283915379\n",
            "train loss:0.036832691243822636\n",
            "train loss:0.024580726402899794\n",
            "train loss:0.018459903155692726\n",
            "train loss:0.029966423035755033\n",
            "train loss:0.01415911170692846\n",
            "train loss:0.119833879137599\n",
            "train loss:0.02646550856676484\n",
            "train loss:0.06547153766511098\n",
            "train loss:0.02455799503400638\n",
            "train loss:0.025869316411392194\n",
            "train loss:0.03739181276962702\n",
            "train loss:0.047292888484989186\n",
            "train loss:0.05101280272401271\n",
            "train loss:0.020509643061193003\n",
            "train loss:0.05010669277249798\n",
            "train loss:0.012660543427190812\n",
            "train loss:0.04578963074517696\n",
            "train loss:0.03777805437760864\n",
            "train loss:0.03663457911021318\n",
            "train loss:0.01577383391469939\n",
            "train loss:0.018633115303159194\n",
            "train loss:0.017232770612620307\n",
            "train loss:0.05425354431219356\n",
            "train loss:0.0262095966926605\n",
            "train loss:0.03828990061407044\n",
            "train loss:0.03884219498060078\n",
            "train loss:0.024627825040773665\n",
            "train loss:0.02648135810627474\n",
            "train loss:0.026637638654705427\n",
            "train loss:0.022957611752566588\n",
            "train loss:0.04118305127611274\n",
            "train loss:0.02401410370984766\n",
            "train loss:0.02168399236763193\n",
            "train loss:0.03062258671762537\n",
            "train loss:0.037214707547457634\n",
            "=== epoch:16, train acc:0.988, test acc:0.956 ===\n",
            "train loss:0.009327616427279868\n",
            "train loss:0.012062882220708655\n",
            "train loss:0.01904380457264462\n",
            "train loss:0.025192477546410047\n",
            "train loss:0.007601100961480994\n",
            "train loss:0.017126234354437955\n",
            "train loss:0.08839285277380116\n",
            "train loss:0.022051686313746776\n",
            "train loss:0.051651390257679036\n",
            "train loss:0.013911837201154003\n",
            "train loss:0.02290174316235257\n",
            "train loss:0.011285930132014914\n",
            "train loss:0.021043337103771608\n",
            "train loss:0.06613131402455057\n",
            "train loss:0.026680095064349915\n",
            "train loss:0.026506137250875375\n",
            "train loss:0.01504035616320027\n",
            "train loss:0.025787871121527726\n",
            "train loss:0.024504672241658053\n",
            "train loss:0.03950118340402418\n",
            "train loss:0.02209602940992872\n",
            "train loss:0.02445716235875068\n",
            "train loss:0.03484472785129388\n",
            "train loss:0.014481558387225866\n",
            "train loss:0.0369726323012223\n",
            "train loss:0.02096543986064796\n",
            "train loss:0.008688186745374239\n",
            "train loss:0.02478927667474709\n",
            "train loss:0.03253845652468048\n",
            "train loss:0.04746129626561848\n",
            "train loss:0.03245546535736347\n",
            "train loss:0.03181786919296628\n",
            "train loss:0.04176370886618168\n",
            "train loss:0.019519027556076228\n",
            "train loss:0.033089975629897884\n",
            "train loss:0.05027792847621059\n",
            "train loss:0.020345306985416162\n",
            "train loss:0.01446987811158444\n",
            "train loss:0.013107427990764808\n",
            "train loss:0.02576976759634205\n",
            "train loss:0.05418997764358444\n",
            "train loss:0.03611512830719532\n",
            "train loss:0.03184069864834145\n",
            "train loss:0.014452562455409101\n",
            "train loss:0.046906135266523236\n",
            "train loss:0.035556792011977686\n",
            "train loss:0.021892701265359815\n",
            "train loss:0.03681584804001522\n",
            "train loss:0.019762273384473172\n",
            "train loss:0.03970573022343323\n",
            "=== epoch:17, train acc:0.985, test acc:0.957 ===\n",
            "train loss:0.03685872171864696\n",
            "train loss:0.020482585699506117\n",
            "train loss:0.014052256111221956\n",
            "train loss:0.02489898578363622\n",
            "train loss:0.009289211267958154\n",
            "train loss:0.05612207942547438\n",
            "train loss:0.06303706160036648\n",
            "train loss:0.018865155541692054\n",
            "train loss:0.024029828381112936\n",
            "train loss:0.023632588458343734\n",
            "train loss:0.01417213150778369\n",
            "train loss:0.04913239835541134\n",
            "train loss:0.03605118159092175\n",
            "train loss:0.0361848485924465\n",
            "train loss:0.01896877640493814\n",
            "train loss:0.058031977251133765\n",
            "train loss:0.011158817793328108\n",
            "train loss:0.029710846267982653\n",
            "train loss:0.05839400736057352\n",
            "train loss:0.03677290550792542\n",
            "train loss:0.031072636092174408\n",
            "train loss:0.05435898215932509\n",
            "train loss:0.024924661566760983\n",
            "train loss:0.012991337041686974\n",
            "train loss:0.03172517897837803\n",
            "train loss:0.013049353983684094\n",
            "train loss:0.012840251774847298\n",
            "train loss:0.0407383902440045\n",
            "train loss:0.01977737698393857\n",
            "train loss:0.06640924340850446\n",
            "train loss:0.046103473579538604\n",
            "train loss:0.01488504872730003\n",
            "train loss:0.01616720793513224\n",
            "train loss:0.05306066696919855\n",
            "train loss:0.02546619687792819\n",
            "train loss:0.0343857890214419\n",
            "train loss:0.022219678177009757\n",
            "train loss:0.017513910137940493\n",
            "train loss:0.01570306015223956\n",
            "train loss:0.01711880693741129\n",
            "train loss:0.007374130932615666\n",
            "train loss:0.025123884193469993\n",
            "train loss:0.05779541064778232\n",
            "train loss:0.01372792603841804\n",
            "train loss:0.012561642435580838\n",
            "train loss:0.01693937450773435\n",
            "train loss:0.015599761116999645\n",
            "train loss:0.024215809179615078\n",
            "train loss:0.042198423630166085\n",
            "train loss:0.020554346300953555\n",
            "=== epoch:18, train acc:0.991, test acc:0.958 ===\n",
            "train loss:0.020196141391962196\n",
            "train loss:0.010340732756982654\n",
            "train loss:0.021714925251775593\n",
            "train loss:0.033927065466781564\n",
            "train loss:0.039596113498944846\n",
            "train loss:0.025706134079156052\n",
            "train loss:0.03618354776022673\n",
            "train loss:0.036949894249212516\n",
            "train loss:0.029094674523107697\n",
            "train loss:0.017186072284086494\n",
            "train loss:0.022281925067079128\n",
            "train loss:0.01990549484403947\n",
            "train loss:0.04708783799128249\n",
            "train loss:0.025937426137517306\n",
            "train loss:0.04348654957525302\n",
            "train loss:0.012102283950334016\n",
            "train loss:0.054312908799493104\n",
            "train loss:0.05369692163299846\n",
            "train loss:0.012752729792619662\n",
            "train loss:0.011725402881793053\n",
            "train loss:0.02878203615530964\n",
            "train loss:0.02955736101952914\n",
            "train loss:0.02770970059501885\n",
            "train loss:0.01889533510465028\n",
            "train loss:0.01833743347071309\n",
            "train loss:0.013790613124348499\n",
            "train loss:0.010667937158957254\n",
            "train loss:0.020455546332133423\n",
            "train loss:0.023077063858551336\n",
            "train loss:0.008917240669587923\n",
            "train loss:0.023110712392162124\n",
            "train loss:0.009607856486108412\n",
            "train loss:0.00944210251807005\n",
            "train loss:0.03094517568738389\n",
            "train loss:0.009707120613634254\n",
            "train loss:0.01576187413451723\n",
            "train loss:0.012417714766578965\n",
            "train loss:0.02973006908138986\n",
            "train loss:0.03437531878865559\n",
            "train loss:0.01613920215707407\n",
            "train loss:0.01812956114947407\n",
            "train loss:0.033837925145084054\n",
            "train loss:0.045027166347439716\n",
            "train loss:0.015597715971056707\n",
            "train loss:0.037044236375538206\n",
            "train loss:0.01213691760381964\n",
            "train loss:0.060747526811693965\n",
            "train loss:0.014719614360300212\n",
            "train loss:0.016651281025324236\n",
            "train loss:0.012100537749286376\n",
            "=== epoch:19, train acc:0.993, test acc:0.953 ===\n",
            "train loss:0.009853606441418658\n",
            "train loss:0.0315748386436799\n",
            "train loss:0.019474592268116094\n",
            "train loss:0.008408357186530149\n",
            "train loss:0.008282580282530188\n",
            "train loss:0.009831429803094406\n",
            "train loss:0.021278718869904692\n",
            "train loss:0.022070858397979926\n",
            "train loss:0.012913158857737082\n",
            "train loss:0.009800793726266705\n",
            "train loss:0.045517083416422424\n",
            "train loss:0.0265074547930154\n",
            "train loss:0.014499989178615964\n",
            "train loss:0.015166726001854099\n",
            "train loss:0.011138957621626627\n",
            "train loss:0.019989150881593257\n",
            "train loss:0.00989699358829133\n",
            "train loss:0.023783026985496655\n",
            "train loss:0.014690557541831017\n",
            "train loss:0.009591592605186015\n",
            "train loss:0.07814324122611717\n",
            "train loss:0.008236410870715733\n",
            "train loss:0.009107096238869019\n",
            "train loss:0.01324844772689163\n",
            "train loss:0.01196157427913201\n",
            "train loss:0.008430795792129195\n",
            "train loss:0.020947429539939107\n",
            "train loss:0.02520707794603683\n",
            "train loss:0.005715810766865833\n",
            "train loss:0.017291099251244887\n",
            "train loss:0.01404028976558413\n",
            "train loss:0.008310231371490567\n",
            "train loss:0.008915229761730117\n",
            "train loss:0.009086800178715037\n",
            "train loss:0.025898438048912373\n",
            "train loss:0.010860771458869621\n",
            "train loss:0.01783411988325861\n",
            "train loss:0.012229176602481328\n",
            "train loss:0.004415278329076556\n",
            "train loss:0.03967309847736107\n",
            "train loss:0.01251096927833183\n",
            "train loss:0.01256224996044291\n",
            "train loss:0.019173416624542167\n",
            "train loss:0.010005508525012535\n",
            "train loss:0.02454845369892401\n",
            "train loss:0.029147224318716414\n",
            "train loss:0.017823061819648142\n",
            "train loss:0.007379293469984286\n",
            "train loss:0.006271911212955788\n",
            "train loss:0.011322937034375726\n",
            "=== epoch:20, train acc:0.995, test acc:0.957 ===\n",
            "train loss:0.056293740712861445\n",
            "train loss:0.05336639723893694\n",
            "train loss:0.010309747234091931\n",
            "train loss:0.028204120789590413\n",
            "train loss:0.010754513741557881\n",
            "train loss:0.01909534951669852\n",
            "train loss:0.01810977607939195\n",
            "train loss:0.07095503017502143\n",
            "train loss:0.01245614586165281\n",
            "train loss:0.021226215081521737\n",
            "train loss:0.01515764892652468\n",
            "train loss:0.025811686469805913\n",
            "train loss:0.015597229426477937\n",
            "train loss:0.030993065383508114\n",
            "train loss:0.014075639706099435\n",
            "train loss:0.006868293335060137\n",
            "train loss:0.03059992900774201\n",
            "train loss:0.031807395140330104\n",
            "train loss:0.0226831024045542\n",
            "train loss:0.01904885219253407\n",
            "train loss:0.014883177026560017\n",
            "train loss:0.02626242223331642\n",
            "train loss:0.04297576633021844\n",
            "train loss:0.010279481513906192\n",
            "train loss:0.019034923927735616\n",
            "train loss:0.015239801822045646\n",
            "train loss:0.015851415508436065\n",
            "train loss:0.022915186703973012\n",
            "train loss:0.018099020749555553\n",
            "train loss:0.04121973200872763\n",
            "train loss:0.012229528131695405\n",
            "train loss:0.026402802832359996\n",
            "train loss:0.011098316739855785\n",
            "train loss:0.013607817633858932\n",
            "train loss:0.01061101380022451\n",
            "train loss:0.009582793994325054\n",
            "train loss:0.013977050195086249\n",
            "train loss:0.012311322181384983\n",
            "train loss:0.0086770757212215\n",
            "train loss:0.019735137518767373\n",
            "train loss:0.020643970815534443\n",
            "train loss:0.005946263475822144\n",
            "train loss:0.012829920331129984\n",
            "train loss:0.026644789323471637\n",
            "train loss:0.010445287308823974\n",
            "train loss:0.010963307962524434\n",
            "train loss:0.015677185851878772\n",
            "train loss:0.01752574110110759\n",
            "train loss:0.02660304299252447\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.9611\n",
            "Saved Network Parameters!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_show(filters, nx=8, margin=3, scale=10):\n",
        "    \"\"\"\n",
        "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
        "    \"\"\"\n",
        "    FN, C, FH, FW = filters.shape\n",
        "    ny = int(np.ceil(FN / nx))\n",
        "\n",
        "    fig = plt.figure()\n",
        "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "    for i in range(FN):\n",
        "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
        "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "network = SimpleConvNet()\n",
        "# 무작위(랜덤) 초기화 후의 가중치\n",
        "filter_show(network.params['W1'])\n",
        "\n",
        "# 학습된 가중치\n",
        "network.load_params(\"params.pkl\")\n",
        "filter_show(network.params['W1'])"
      ],
      "metadata": {
        "id": "tQRhjuX1AZ1o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "outputId": "d2164708-6fbf-4b5f-9b01-03ee3844a6f2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 30 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 30 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ueCYbZ03C03W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}