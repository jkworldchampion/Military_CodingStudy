{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "basic_deeplearning_7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOmYT0C416H8ej7XsufZwdA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkworldchampion/Military_CodingStudy/blob/main/deeplearning/basic_deeplearning_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 합성곱 신경망(CNN)"
      ],
      "metadata": {
        "id": "Jif5qm3pqiku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번 장의 주제는 합성곱 신경망입니다.  \n",
        " CNN은 이미지 인식과 음성 인식 등 다양한 곳에서 사용되는데,   \n",
        " 특히 이미지 인식 분야에서 딥러닝을 활용한 기법은 대부분 CNN을 기반으로 합니다.   \n",
        " 이번장에서는 CNN의 매커니즘을 자세히 알아봅시다."
      ],
      "metadata": {
        "id": "eSc6hgZAqhx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1 전체 구조\n",
        "CNN도 지금까지와 같이 레고 블럭처럼 조합하여 만들 수 있습니다. 다만, 합성곱 계층과 풀링계층이 새롭게 등장합니다. 지금까지의 신경망은 인접하는 계층의 모든 뉴런과 결합되어 있습니다. 이를 완전연결(Fully-Connected)이라고 하며, Affine 계층이라는 이름으로 구현했습니다. 완전연결 신경망은 Affine 계층 뒤에 활성화 함수를 갖는 ReLU 계층이 이어집니다.  \n",
        "이러한 연결이 Conv-ReLU-(Pooling)을 바뀌었다고 생각하면 됩니다. "
      ],
      "metadata": {
        "id": "iF8fMsLYrACw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM1mT5yvs9hL",
        "outputId": "4fb865f9-a01a-4769-8df2-3f52018dbc0c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Colab_Notebooks/Machine_learning/book/basic_deeplearning/deep-learning-from-scratch/ch07"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3CDWzNJtB1p",
        "outputId": "69cb6514-5c00-443f-b1b7-d0c63bb72821"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab_Notebooks/Machine_learning/book/basic_deeplearning/deep-learning-from-scratch/ch07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from simple_convnet import SimpleConvNet\n",
        "from matplotlib.image import imread\n",
        "from common.layers import *\n",
        "import pickle\n",
        "from collections import OrderedDict\n",
        "from common.gradient import numerical_gradient\n",
        "from dataset.mnist import load_mnist\n",
        "from simple_convnet import SimpleConvNet\n",
        "from common.trainer import Trainer"
      ],
      "metadata": {
        "id": "-yuWfeuhs35j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 합성곱 계층 구현하기\n",
        "from common.util import im2col\n",
        "x1 = np.random.rand(1, 3, 7, 7).round(2)\n",
        "print(x1)\n",
        "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
        "print(col1)\n",
        "print(col1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLV-IKHZAlOr",
        "outputId": "146c784f-e67d-4255-fc7e-bad30b916040"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[0.91 0.17 0.21 0.54 0.64 0.55 0.08]\n",
            "   [0.25 0.61 0.   0.35 0.24 0.09 0.19]\n",
            "   [0.69 0.8  0.51 0.65 0.31 0.22 0.26]\n",
            "   [0.6  0.06 0.57 0.98 0.1  0.25 0.91]\n",
            "   [0.88 0.53 0.75 0.41 0.13 0.07 0.23]\n",
            "   [0.56 0.51 0.4  0.63 0.95 0.51 0.78]\n",
            "   [0.96 0.01 0.16 0.59 0.19 0.4  0.76]]\n",
            "\n",
            "  [[0.6  0.48 0.45 0.55 0.99 0.04 0.73]\n",
            "   [0.8  0.62 0.65 0.19 0.34 0.45 0.24]\n",
            "   [0.4  0.76 0.32 0.54 0.52 0.24 0.93]\n",
            "   [0.6  0.64 0.74 0.69 0.89 0.07 0.86]\n",
            "   [0.85 0.36 0.52 0.28 0.18 0.26 0.86]\n",
            "   [0.68 0.97 0.4  0.78 0.03 0.03 0.14]\n",
            "   [0.12 0.62 0.09 0.71 0.7  0.29 0.11]]\n",
            "\n",
            "  [[0.15 0.89 0.48 0.35 0.3  0.87 0.49]\n",
            "   [0.09 0.36 0.38 0.33 0.12 0.9  0.27]\n",
            "   [0.46 0.63 0.36 0.26 0.23 0.56 0.12]\n",
            "   [0.61 0.22 0.67 0.31 0.67 0.47 0.43]\n",
            "   [0.9  0.76 0.5  0.23 0.18 0.05 0.54]\n",
            "   [0.63 0.8  0.61 0.11 0.73 0.49 0.64]\n",
            "   [0.48 0.62 0.54 0.07 0.62 0.6  0.37]]]]\n",
            "[[0.91 0.17 0.21 0.54 0.64 0.25 0.61 0.   0.35 0.24 0.69 0.8  0.51 0.65\n",
            "  0.31 0.6  0.06 0.57 0.98 0.1  0.88 0.53 0.75 0.41 0.13 0.6  0.48 0.45\n",
            "  0.55 0.99 0.8  0.62 0.65 0.19 0.34 0.4  0.76 0.32 0.54 0.52 0.6  0.64\n",
            "  0.74 0.69 0.89 0.85 0.36 0.52 0.28 0.18 0.15 0.89 0.48 0.35 0.3  0.09\n",
            "  0.36 0.38 0.33 0.12 0.46 0.63 0.36 0.26 0.23 0.61 0.22 0.67 0.31 0.67\n",
            "  0.9  0.76 0.5  0.23 0.18]\n",
            " [0.17 0.21 0.54 0.64 0.55 0.61 0.   0.35 0.24 0.09 0.8  0.51 0.65 0.31\n",
            "  0.22 0.06 0.57 0.98 0.1  0.25 0.53 0.75 0.41 0.13 0.07 0.48 0.45 0.55\n",
            "  0.99 0.04 0.62 0.65 0.19 0.34 0.45 0.76 0.32 0.54 0.52 0.24 0.64 0.74\n",
            "  0.69 0.89 0.07 0.36 0.52 0.28 0.18 0.26 0.89 0.48 0.35 0.3  0.87 0.36\n",
            "  0.38 0.33 0.12 0.9  0.63 0.36 0.26 0.23 0.56 0.22 0.67 0.31 0.67 0.47\n",
            "  0.76 0.5  0.23 0.18 0.05]\n",
            " [0.21 0.54 0.64 0.55 0.08 0.   0.35 0.24 0.09 0.19 0.51 0.65 0.31 0.22\n",
            "  0.26 0.57 0.98 0.1  0.25 0.91 0.75 0.41 0.13 0.07 0.23 0.45 0.55 0.99\n",
            "  0.04 0.73 0.65 0.19 0.34 0.45 0.24 0.32 0.54 0.52 0.24 0.93 0.74 0.69\n",
            "  0.89 0.07 0.86 0.52 0.28 0.18 0.26 0.86 0.48 0.35 0.3  0.87 0.49 0.38\n",
            "  0.33 0.12 0.9  0.27 0.36 0.26 0.23 0.56 0.12 0.67 0.31 0.67 0.47 0.43\n",
            "  0.5  0.23 0.18 0.05 0.54]\n",
            " [0.25 0.61 0.   0.35 0.24 0.69 0.8  0.51 0.65 0.31 0.6  0.06 0.57 0.98\n",
            "  0.1  0.88 0.53 0.75 0.41 0.13 0.56 0.51 0.4  0.63 0.95 0.8  0.62 0.65\n",
            "  0.19 0.34 0.4  0.76 0.32 0.54 0.52 0.6  0.64 0.74 0.69 0.89 0.85 0.36\n",
            "  0.52 0.28 0.18 0.68 0.97 0.4  0.78 0.03 0.09 0.36 0.38 0.33 0.12 0.46\n",
            "  0.63 0.36 0.26 0.23 0.61 0.22 0.67 0.31 0.67 0.9  0.76 0.5  0.23 0.18\n",
            "  0.63 0.8  0.61 0.11 0.73]\n",
            " [0.61 0.   0.35 0.24 0.09 0.8  0.51 0.65 0.31 0.22 0.06 0.57 0.98 0.1\n",
            "  0.25 0.53 0.75 0.41 0.13 0.07 0.51 0.4  0.63 0.95 0.51 0.62 0.65 0.19\n",
            "  0.34 0.45 0.76 0.32 0.54 0.52 0.24 0.64 0.74 0.69 0.89 0.07 0.36 0.52\n",
            "  0.28 0.18 0.26 0.97 0.4  0.78 0.03 0.03 0.36 0.38 0.33 0.12 0.9  0.63\n",
            "  0.36 0.26 0.23 0.56 0.22 0.67 0.31 0.67 0.47 0.76 0.5  0.23 0.18 0.05\n",
            "  0.8  0.61 0.11 0.73 0.49]\n",
            " [0.   0.35 0.24 0.09 0.19 0.51 0.65 0.31 0.22 0.26 0.57 0.98 0.1  0.25\n",
            "  0.91 0.75 0.41 0.13 0.07 0.23 0.4  0.63 0.95 0.51 0.78 0.65 0.19 0.34\n",
            "  0.45 0.24 0.32 0.54 0.52 0.24 0.93 0.74 0.69 0.89 0.07 0.86 0.52 0.28\n",
            "  0.18 0.26 0.86 0.4  0.78 0.03 0.03 0.14 0.38 0.33 0.12 0.9  0.27 0.36\n",
            "  0.26 0.23 0.56 0.12 0.67 0.31 0.67 0.47 0.43 0.5  0.23 0.18 0.05 0.54\n",
            "  0.61 0.11 0.73 0.49 0.64]\n",
            " [0.69 0.8  0.51 0.65 0.31 0.6  0.06 0.57 0.98 0.1  0.88 0.53 0.75 0.41\n",
            "  0.13 0.56 0.51 0.4  0.63 0.95 0.96 0.01 0.16 0.59 0.19 0.4  0.76 0.32\n",
            "  0.54 0.52 0.6  0.64 0.74 0.69 0.89 0.85 0.36 0.52 0.28 0.18 0.68 0.97\n",
            "  0.4  0.78 0.03 0.12 0.62 0.09 0.71 0.7  0.46 0.63 0.36 0.26 0.23 0.61\n",
            "  0.22 0.67 0.31 0.67 0.9  0.76 0.5  0.23 0.18 0.63 0.8  0.61 0.11 0.73\n",
            "  0.48 0.62 0.54 0.07 0.62]\n",
            " [0.8  0.51 0.65 0.31 0.22 0.06 0.57 0.98 0.1  0.25 0.53 0.75 0.41 0.13\n",
            "  0.07 0.51 0.4  0.63 0.95 0.51 0.01 0.16 0.59 0.19 0.4  0.76 0.32 0.54\n",
            "  0.52 0.24 0.64 0.74 0.69 0.89 0.07 0.36 0.52 0.28 0.18 0.26 0.97 0.4\n",
            "  0.78 0.03 0.03 0.62 0.09 0.71 0.7  0.29 0.63 0.36 0.26 0.23 0.56 0.22\n",
            "  0.67 0.31 0.67 0.47 0.76 0.5  0.23 0.18 0.05 0.8  0.61 0.11 0.73 0.49\n",
            "  0.62 0.54 0.07 0.62 0.6 ]\n",
            " [0.51 0.65 0.31 0.22 0.26 0.57 0.98 0.1  0.25 0.91 0.75 0.41 0.13 0.07\n",
            "  0.23 0.4  0.63 0.95 0.51 0.78 0.16 0.59 0.19 0.4  0.76 0.32 0.54 0.52\n",
            "  0.24 0.93 0.74 0.69 0.89 0.07 0.86 0.52 0.28 0.18 0.26 0.86 0.4  0.78\n",
            "  0.03 0.03 0.14 0.09 0.71 0.7  0.29 0.11 0.36 0.26 0.23 0.56 0.12 0.67\n",
            "  0.31 0.67 0.47 0.43 0.5  0.23 0.18 0.05 0.54 0.61 0.11 0.73 0.49 0.64\n",
            "  0.54 0.07 0.62 0.6  0.37]]\n",
            "(9, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = np.random.rand(10, 3, 7, 7).round(2)\n",
        "print(x2)\n",
        "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
        "print(col2)\n",
        "print(col2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV5NiI5rFZDK",
        "outputId": "ef81915d-d6e7-477f-cbbb-32279d53dcee"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[0.46 0.73 0.54 ... 0.27 0.53 0.37]\n",
            "   [0.27 0.54 0.44 ... 0.86 0.31 0.4 ]\n",
            "   [0.7  0.56 0.14 ... 0.73 0.1  0.31]\n",
            "   ...\n",
            "   [0.27 0.22 0.94 ... 0.13 0.52 0.11]\n",
            "   [0.9  0.81 0.72 ... 0.55 0.08 0.6 ]\n",
            "   [0.38 0.71 0.36 ... 0.58 0.35 0.56]]\n",
            "\n",
            "  [[0.09 0.81 0.22 ... 0.8  0.29 0.67]\n",
            "   [0.32 0.89 1.   ... 0.09 0.54 0.16]\n",
            "   [0.52 0.83 0.66 ... 0.53 0.71 0.73]\n",
            "   ...\n",
            "   [0.92 0.69 0.58 ... 0.19 0.72 0.94]\n",
            "   [0.67 0.79 0.63 ... 0.69 0.36 0.95]\n",
            "   [0.65 0.09 0.99 ... 0.77 0.68 0.39]]\n",
            "\n",
            "  [[0.19 0.66 0.03 ... 0.89 0.7  0.47]\n",
            "   [0.65 0.34 0.88 ... 0.76 0.82 0.28]\n",
            "   [0.71 0.89 0.41 ... 0.8  0.55 0.12]\n",
            "   ...\n",
            "   [0.36 0.64 0.5  ... 0.01 0.2  0.78]\n",
            "   [0.09 1.   0.4  ... 0.37 0.81 0.08]\n",
            "   [0.58 0.7  0.28 ... 0.3  0.28 0.02]]]\n",
            "\n",
            "\n",
            " [[[0.64 0.24 0.31 ... 1.   0.05 0.59]\n",
            "   [0.77 0.64 0.35 ... 0.79 0.59 0.04]\n",
            "   [0.13 0.84 0.88 ... 0.91 0.55 0.25]\n",
            "   ...\n",
            "   [0.81 0.67 0.84 ... 0.69 0.55 0.99]\n",
            "   [0.28 0.66 0.4  ... 0.92 0.74 0.11]\n",
            "   [0.95 0.1  0.51 ... 0.66 0.04 0.83]]\n",
            "\n",
            "  [[0.03 0.23 0.72 ... 0.68 0.6  0.71]\n",
            "   [0.86 0.3  0.81 ... 0.34 0.72 0.33]\n",
            "   [0.16 0.73 0.18 ... 0.07 0.74 0.79]\n",
            "   ...\n",
            "   [0.59 0.64 0.55 ... 0.8  0.28 0.99]\n",
            "   [0.81 0.02 0.78 ... 0.34 0.75 0.61]\n",
            "   [0.03 0.44 0.71 ... 0.02 0.44 0.35]]\n",
            "\n",
            "  [[0.77 0.99 0.82 ... 0.61 0.86 0.91]\n",
            "   [0.63 0.82 0.31 ... 0.91 0.81 0.3 ]\n",
            "   [0.69 0.12 0.35 ... 0.07 0.08 0.01]\n",
            "   ...\n",
            "   [0.97 0.46 0.02 ... 0.99 0.05 0.24]\n",
            "   [0.99 0.08 0.13 ... 0.23 0.22 0.04]\n",
            "   [0.54 0.54 0.71 ... 0.89 0.46 0.16]]]\n",
            "\n",
            "\n",
            " [[[0.72 0.58 0.94 ... 0.04 0.14 0.6 ]\n",
            "   [0.25 0.95 0.48 ... 0.95 0.46 0.05]\n",
            "   [0.07 0.93 0.98 ... 0.13 0.6  0.22]\n",
            "   ...\n",
            "   [0.6  1.   0.6  ... 0.56 0.59 0.99]\n",
            "   [0.6  0.77 0.68 ... 0.15 0.7  0.22]\n",
            "   [0.15 0.96 0.34 ... 0.2  0.83 0.58]]\n",
            "\n",
            "  [[0.38 0.26 0.33 ... 0.82 0.45 0.84]\n",
            "   [0.09 0.62 0.63 ... 0.45 0.58 0.89]\n",
            "   [0.76 0.54 0.82 ... 0.72 0.17 0.3 ]\n",
            "   ...\n",
            "   [0.22 0.04 0.73 ... 0.53 0.57 0.74]\n",
            "   [0.86 0.43 0.97 ... 0.19 0.98 0.31]\n",
            "   [0.76 0.77 0.54 ... 0.33 0.75 0.01]]\n",
            "\n",
            "  [[0.43 0.31 0.23 ... 0.96 0.59 0.27]\n",
            "   [0.84 0.74 0.87 ... 0.48 0.26 0.08]\n",
            "   [0.72 0.4  0.58 ... 0.9  0.41 0.4 ]\n",
            "   ...\n",
            "   [0.96 0.18 0.4  ... 0.69 0.06 0.96]\n",
            "   [0.56 0.67 0.84 ... 0.55 0.03 0.93]\n",
            "   [0.41 0.25 0.62 ... 0.52 0.5  0.78]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[0.42 0.68 0.42 ... 0.22 0.17 0.03]\n",
            "   [0.86 0.64 0.88 ... 0.25 0.57 0.3 ]\n",
            "   [0.98 0.68 0.65 ... 0.43 0.4  0.56]\n",
            "   ...\n",
            "   [0.33 0.8  0.29 ... 0.65 0.81 0.45]\n",
            "   [0.76 0.34 0.75 ... 0.27 0.08 0.23]\n",
            "   [0.02 0.63 0.13 ... 0.35 0.04 0.42]]\n",
            "\n",
            "  [[0.29 0.53 0.91 ... 0.61 0.6  0.31]\n",
            "   [0.39 0.83 0.84 ... 0.69 0.78 0.88]\n",
            "   [0.5  0.41 0.96 ... 0.37 0.25 0.72]\n",
            "   ...\n",
            "   [0.87 0.42 0.9  ... 0.99 0.22 0.6 ]\n",
            "   [0.77 0.09 0.32 ... 0.91 0.22 0.8 ]\n",
            "   [0.65 0.97 0.05 ... 0.17 0.96 0.14]]\n",
            "\n",
            "  [[0.51 0.58 0.07 ... 0.25 0.83 0.62]\n",
            "   [0.01 0.73 1.   ... 0.29 0.49 0.24]\n",
            "   [0.93 0.64 0.42 ... 0.94 0.28 0.88]\n",
            "   ...\n",
            "   [0.31 0.99 0.1  ... 0.84 0.05 0.3 ]\n",
            "   [0.08 0.34 0.61 ... 0.53 0.54 0.28]\n",
            "   [0.79 0.14 0.91 ... 0.68 0.44 0.59]]]\n",
            "\n",
            "\n",
            " [[[0.75 0.44 0.92 ... 0.02 0.14 0.48]\n",
            "   [0.15 0.48 0.21 ... 0.3  0.33 0.24]\n",
            "   [0.82 0.04 0.75 ... 0.89 0.14 0.38]\n",
            "   ...\n",
            "   [0.11 0.31 0.69 ... 0.7  0.07 0.81]\n",
            "   [0.5  0.17 0.36 ... 0.24 0.94 0.69]\n",
            "   [0.94 0.15 0.48 ... 0.74 0.15 0.1 ]]\n",
            "\n",
            "  [[0.97 0.74 0.18 ... 0.18 0.84 0.81]\n",
            "   [0.09 0.62 0.09 ... 0.25 0.45 0.37]\n",
            "   [0.28 0.61 0.55 ... 0.64 0.9  0.35]\n",
            "   ...\n",
            "   [0.55 0.82 0.64 ... 0.32 0.19 0.42]\n",
            "   [0.78 0.22 0.25 ... 0.7  0.18 0.58]\n",
            "   [0.84 0.49 0.64 ... 0.54 0.83 0.56]]\n",
            "\n",
            "  [[0.48 0.85 0.01 ... 0.56 0.38 0.05]\n",
            "   [0.59 0.85 0.68 ... 0.83 0.25 0.51]\n",
            "   [0.36 0.04 0.53 ... 0.23 0.75 0.05]\n",
            "   ...\n",
            "   [0.59 0.61 0.7  ... 0.5  0.07 0.12]\n",
            "   [0.27 0.18 0.26 ... 0.07 0.18 0.36]\n",
            "   [0.46 0.03 0.42 ... 0.63 0.4  0.47]]]\n",
            "\n",
            "\n",
            " [[[0.95 0.49 0.3  ... 0.24 0.1  0.83]\n",
            "   [0.04 0.36 0.35 ... 0.75 0.18 0.59]\n",
            "   [0.63 0.84 0.22 ... 0.17 0.57 0.79]\n",
            "   ...\n",
            "   [0.76 0.66 0.11 ... 0.79 0.53 0.47]\n",
            "   [0.62 0.16 0.81 ... 0.72 0.85 0.4 ]\n",
            "   [0.54 0.55 0.28 ... 0.91 0.28 0.3 ]]\n",
            "\n",
            "  [[0.27 0.78 0.09 ... 0.94 0.2  0.67]\n",
            "   [0.81 1.   0.43 ... 0.94 0.53 0.24]\n",
            "   [0.03 0.79 0.39 ... 0.46 0.11 0.52]\n",
            "   ...\n",
            "   [0.12 0.11 0.1  ... 0.06 0.32 0.24]\n",
            "   [0.01 0.46 0.66 ... 0.6  0.5  0.95]\n",
            "   [0.61 0.59 0.28 ... 0.51 0.41 0.28]]\n",
            "\n",
            "  [[0.84 0.07 0.38 ... 0.4  0.94 0.49]\n",
            "   [0.39 0.59 0.77 ... 0.29 0.61 0.65]\n",
            "   [0.72 0.71 0.11 ... 0.54 0.64 0.9 ]\n",
            "   ...\n",
            "   [0.45 0.83 0.56 ... 0.94 0.96 0.94]\n",
            "   [0.04 0.07 0.48 ... 0.42 0.32 0.61]\n",
            "   [0.96 0.8  0.86 ... 0.21 0.5  0.59]]]]\n",
            "[[0.46 0.73 0.54 ... 0.5  0.85 0.01]\n",
            " [0.73 0.54 0.99 ... 0.85 0.01 0.2 ]\n",
            " [0.54 0.99 0.27 ... 0.01 0.2  0.78]\n",
            " ...\n",
            " [0.63 0.84 0.22 ... 0.86 0.79 0.21]\n",
            " [0.84 0.22 0.37 ... 0.79 0.21 0.5 ]\n",
            " [0.22 0.37 0.17 ... 0.21 0.5  0.59]]\n",
            "(90, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleConvNet:\n",
        "    \"\"\"단순한 합성곱 신경망\n",
        "    \n",
        "    conv - relu - pool - affine - relu - affine - softmax\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
        "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
        "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
        "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
        "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
        "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
        "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(1, 28, 28),  # 784\n",
        "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "        # 가중치 매개변수 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "                            np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        self.params['W3'] = weight_init_std * \\\n",
        "                            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()   # 순서가 있는 딕셔너리\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                           conv_param['stride'], conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \"\"\"손실 함수를 구한다.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        \"\"\"\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        \n",
        "        acc = 0.0\n",
        "        \n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt) \n",
        "        \n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다（수치미분）.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        loss_w = lambda w: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in (1, 2, 3):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다(오차역전파법).\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "        \n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
        "            self.layers[key].W = self.params['W' + str(i+1)]\n",
        "            self.layers[key].b = self.params['b' + str(i+1)]\n"
      ],
      "metadata": {
        "id": "AT1SDylktkze"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
        "x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28), \n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "                        \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# 매개변수 보존\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "07-RaFBZ84Cb",
        "outputId": "fdd7c482-f11f-4350-b53a-8eba51f8ace8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss:2.2996416094725767\n",
            "=== epoch:1, train acc:0.262, test acc:0.283 ===\n",
            "train loss:2.297073463937026\n",
            "train loss:2.293917146280926\n",
            "train loss:2.2877936959590714\n",
            "train loss:2.281836757599186\n",
            "train loss:2.2661258976479375\n",
            "train loss:2.259244317938249\n",
            "train loss:2.241641256062613\n",
            "train loss:2.2045635888905544\n",
            "train loss:2.200354318495981\n",
            "train loss:2.168837067714892\n",
            "train loss:2.1236408088904666\n",
            "train loss:2.084186583283051\n",
            "train loss:2.0503192736073172\n",
            "train loss:1.9842885118832512\n",
            "train loss:1.904655199942478\n",
            "train loss:1.932845182463631\n",
            "train loss:1.7633686290842865\n",
            "train loss:1.6689870567127998\n",
            "train loss:1.6678801564425578\n",
            "train loss:1.5490282857606599\n",
            "train loss:1.4794955954577744\n",
            "train loss:1.518838049724867\n",
            "train loss:1.3505461428753625\n",
            "train loss:1.3048835311099511\n",
            "train loss:1.282806195397228\n",
            "train loss:1.1902048381853645\n",
            "train loss:1.0304129709316192\n",
            "train loss:0.9816742520953855\n",
            "train loss:0.9543925539452449\n",
            "train loss:0.8782713586092723\n",
            "train loss:0.829106065438946\n",
            "train loss:0.7675611586540358\n",
            "train loss:0.8688276613535864\n",
            "train loss:0.6579124773139648\n",
            "train loss:0.8269581593316329\n",
            "train loss:0.688890026863624\n",
            "train loss:0.6264840402969913\n",
            "train loss:0.6102280869471163\n",
            "train loss:0.5772143615768097\n",
            "train loss:0.7065676608685308\n",
            "train loss:0.6822081628930623\n",
            "train loss:0.6398835086119786\n",
            "train loss:0.7512179290675004\n",
            "train loss:0.5911524253297881\n",
            "train loss:0.5310483882765358\n",
            "train loss:0.46849394826381363\n",
            "train loss:0.5779369684959089\n",
            "train loss:0.6321489207529268\n",
            "train loss:0.5845158615703453\n",
            "train loss:0.48089125430308965\n",
            "=== epoch:2, train acc:0.83, test acc:0.795 ===\n",
            "train loss:0.5347742463399171\n",
            "train loss:0.4604065645104377\n",
            "train loss:0.5085512132772573\n",
            "train loss:0.471973211357406\n",
            "train loss:0.36180866530501005\n",
            "train loss:0.5056954604112783\n",
            "train loss:0.630588309769666\n",
            "train loss:0.3639331060433296\n",
            "train loss:0.45600697658237554\n",
            "train loss:0.4061489307999275\n",
            "train loss:0.432889842616434\n",
            "train loss:0.3884407684361777\n",
            "train loss:0.4155139165828094\n",
            "train loss:0.42588442621398015\n",
            "train loss:0.4597561749603195\n",
            "train loss:0.44197453944405596\n",
            "train loss:0.3049454706220758\n",
            "train loss:0.2536539571034926\n",
            "train loss:0.33354171303949776\n",
            "train loss:0.38433534241509465\n",
            "train loss:0.2535050431893613\n",
            "train loss:0.3194431608999963\n",
            "train loss:0.29695465067275695\n",
            "train loss:0.3237122338923137\n",
            "train loss:0.25541350725214607\n",
            "train loss:0.4519416082022265\n",
            "train loss:0.33620630616809827\n",
            "train loss:0.34442844490837776\n",
            "train loss:0.32156948732385837\n",
            "train loss:0.47362259659356104\n",
            "train loss:0.3635870092109762\n",
            "train loss:0.43876141645529587\n",
            "train loss:0.4395767674199261\n",
            "train loss:0.2622484657228843\n",
            "train loss:0.38033185542556036\n",
            "train loss:0.38950790973732535\n",
            "train loss:0.2363785500794732\n",
            "train loss:0.3203146230847308\n",
            "train loss:0.2736587011019738\n",
            "train loss:0.3529451264351372\n",
            "train loss:0.3806122026455614\n",
            "train loss:0.6297940695809858\n",
            "train loss:0.2619995514722478\n",
            "train loss:0.30382035716635586\n",
            "train loss:0.27528132943173234\n",
            "train loss:0.4294678115640264\n",
            "train loss:0.4552517364878834\n",
            "train loss:0.2707213004668932\n",
            "train loss:0.3374136491112925\n",
            "train loss:0.1765395128423596\n",
            "=== epoch:3, train acc:0.879, test acc:0.872 ===\n",
            "train loss:0.32147963772959615\n",
            "train loss:0.6137994116083502\n",
            "train loss:0.44438720497634726\n",
            "train loss:0.21562058072570178\n",
            "train loss:0.40921301221673034\n",
            "train loss:0.4163314907367288\n",
            "train loss:0.24830318399975568\n",
            "train loss:0.35676300405661293\n",
            "train loss:0.4326848476592633\n",
            "train loss:0.3660703040395794\n",
            "train loss:0.36130462556261805\n",
            "train loss:0.2510789232199475\n",
            "train loss:0.41491653345931745\n",
            "train loss:0.44771118063584514\n",
            "train loss:0.475005260680946\n",
            "train loss:0.16068080651728744\n",
            "train loss:0.2833782580428457\n",
            "train loss:0.5063859990340206\n",
            "train loss:0.40249590952498265\n",
            "train loss:0.45118935561229057\n",
            "train loss:0.2802824790495414\n",
            "train loss:0.2917820484791815\n",
            "train loss:0.3349720666009243\n",
            "train loss:0.26664370563199086\n",
            "train loss:0.27051587799994875\n",
            "train loss:0.2925822004385494\n",
            "train loss:0.29202986135952136\n",
            "train loss:0.2873274585088744\n",
            "train loss:0.33150480037101304\n",
            "train loss:0.5183805458215709\n",
            "train loss:0.3903470914953518\n",
            "train loss:0.33138586482860327\n",
            "train loss:0.31789056870018007\n",
            "train loss:0.34626175680911897\n",
            "train loss:0.220477557576082\n",
            "train loss:0.30618346555929316\n",
            "train loss:0.39176032086304585\n",
            "train loss:0.3410690016188001\n",
            "train loss:0.32638713264527197\n",
            "train loss:0.2739169471975271\n",
            "train loss:0.19725683796131718\n",
            "train loss:0.26286466066997116\n",
            "train loss:0.23660245377024933\n",
            "train loss:0.3272072459665003\n",
            "train loss:0.4642082321629136\n",
            "train loss:0.3050814339604641\n",
            "train loss:0.2482245942027882\n",
            "train loss:0.21519694820473537\n",
            "train loss:0.29650798646737414\n",
            "train loss:0.33344949953040354\n",
            "=== epoch:4, train acc:0.889, test acc:0.875 ===\n",
            "train loss:0.4000763339851149\n",
            "train loss:0.3696656443459206\n",
            "train loss:0.46856360381233203\n",
            "train loss:0.3377171504284938\n",
            "train loss:0.18583915141756602\n",
            "train loss:0.3228123545641363\n",
            "train loss:0.5046826625838374\n",
            "train loss:0.4233248940550046\n",
            "train loss:0.22205146157063488\n",
            "train loss:0.3662781318921251\n",
            "train loss:0.4326775329727034\n",
            "train loss:0.2851729466884022\n",
            "train loss:0.3703854201483026\n",
            "train loss:0.4299095416061225\n",
            "train loss:0.3444628931917139\n",
            "train loss:0.29506289837565697\n",
            "train loss:0.16714227689705063\n",
            "train loss:0.37090843164361026\n",
            "train loss:0.18375733813888648\n",
            "train loss:0.2982765802367016\n",
            "train loss:0.3077423686717208\n",
            "train loss:0.24795658056243394\n",
            "train loss:0.17256555205651075\n",
            "train loss:0.15308758853052634\n",
            "train loss:0.44625191469956255\n",
            "train loss:0.34457368808586025\n",
            "train loss:0.2400423547847603\n",
            "train loss:0.3045610930033687\n",
            "train loss:0.3137510560771207\n",
            "train loss:0.43561103426355463\n",
            "train loss:0.2454288547308677\n",
            "train loss:0.32315358607922273\n",
            "train loss:0.3276462147136733\n",
            "train loss:0.3079500548124795\n",
            "train loss:0.3844305016216976\n",
            "train loss:0.19627809187222425\n",
            "train loss:0.22978525646174677\n",
            "train loss:0.35224203182675917\n",
            "train loss:0.2216513711068428\n",
            "train loss:0.2290400285679695\n",
            "train loss:0.4008977287481618\n",
            "train loss:0.21168922955358238\n",
            "train loss:0.19188643944192044\n",
            "train loss:0.3929874839962844\n",
            "train loss:0.2327511464139068\n",
            "train loss:0.28849536676103\n",
            "train loss:0.21758440625262335\n",
            "train loss:0.24544084801532373\n",
            "train loss:0.16305917521240754\n",
            "train loss:0.3290483456905225\n",
            "=== epoch:5, train acc:0.897, test acc:0.906 ===\n",
            "train loss:0.11452481505330409\n",
            "train loss:0.2138514566781795\n",
            "train loss:0.19851914906420678\n",
            "train loss:0.1633258421410337\n",
            "train loss:0.20471696480848256\n",
            "train loss:0.2822291825573678\n",
            "train loss:0.14291966353149943\n",
            "train loss:0.2077715668146779\n",
            "train loss:0.17796010820185934\n",
            "train loss:0.1889182780150608\n",
            "train loss:0.24108113366776568\n",
            "train loss:0.3244092768407408\n",
            "train loss:0.24222612584291972\n",
            "train loss:0.2927255104151694\n",
            "train loss:0.40230619357209507\n",
            "train loss:0.1783662998214514\n",
            "train loss:0.2088252873521535\n",
            "train loss:0.27374161280333537\n",
            "train loss:0.08054568314357931\n",
            "train loss:0.18320576867034735\n",
            "train loss:0.26310700243637025\n",
            "train loss:0.24409659618194504\n",
            "train loss:0.1968590219965381\n",
            "train loss:0.357911800446292\n",
            "train loss:0.35224707114446024\n",
            "train loss:0.18927943103084202\n",
            "train loss:0.14700274932490948\n",
            "train loss:0.26285359820139026\n",
            "train loss:0.28179756587429855\n",
            "train loss:0.1532796046066901\n",
            "train loss:0.17582642498729292\n",
            "train loss:0.23780697230044948\n",
            "train loss:0.245306109694683\n",
            "train loss:0.213990851016592\n",
            "train loss:0.33061535391714203\n",
            "train loss:0.24821305100596475\n",
            "train loss:0.3259188424094057\n",
            "train loss:0.279163729116569\n",
            "train loss:0.2574356159174895\n",
            "train loss:0.2287356889379392\n",
            "train loss:0.1445344718797273\n",
            "train loss:0.22027396867095253\n",
            "train loss:0.22643943587129403\n",
            "train loss:0.24313951815175489\n",
            "train loss:0.11180036992259308\n",
            "train loss:0.1678668366360895\n",
            "train loss:0.19216933560159175\n",
            "train loss:0.16105967607669855\n",
            "train loss:0.23448661046174266\n",
            "train loss:0.18225643696430158\n",
            "=== epoch:6, train acc:0.903, test acc:0.902 ===\n",
            "train loss:0.2981876730216393\n",
            "train loss:0.15034746448971625\n",
            "train loss:0.0959614048789888\n",
            "train loss:0.3186561753126602\n",
            "train loss:0.20114992619523633\n",
            "train loss:0.23910530933614385\n",
            "train loss:0.36485788488901966\n",
            "train loss:0.1498640199809366\n",
            "train loss:0.3065244910001801\n",
            "train loss:0.19173469433238416\n",
            "train loss:0.36284850100052346\n",
            "train loss:0.18660684122607918\n",
            "train loss:0.2758111854750117\n",
            "train loss:0.1831313972197588\n",
            "train loss:0.15895071580896833\n",
            "train loss:0.20116213158881874\n",
            "train loss:0.1807242582062341\n",
            "train loss:0.3410827156631139\n",
            "train loss:0.2974492864410716\n",
            "train loss:0.2145566417257746\n",
            "train loss:0.20671345618127604\n",
            "train loss:0.22324488325981398\n",
            "train loss:0.1749770854335026\n",
            "train loss:0.3324700251801421\n",
            "train loss:0.1797141242021666\n",
            "train loss:0.2534922657680466\n",
            "train loss:0.20918028614773262\n",
            "train loss:0.24391145605725095\n",
            "train loss:0.2943866641367784\n",
            "train loss:0.18113391716442442\n",
            "train loss:0.164737190030957\n",
            "train loss:0.17097545528697877\n",
            "train loss:0.284284872938692\n",
            "train loss:0.2010663306317189\n",
            "train loss:0.1478735089255559\n",
            "train loss:0.16084609502629432\n",
            "train loss:0.18439612273148914\n",
            "train loss:0.09948571590177469\n",
            "train loss:0.15833723484095463\n",
            "train loss:0.2387607682088612\n",
            "train loss:0.25071299158755755\n",
            "train loss:0.2209775995080393\n",
            "train loss:0.2026453887749599\n",
            "train loss:0.13494732358649267\n",
            "train loss:0.25805823721601695\n",
            "train loss:0.21085126677689792\n",
            "train loss:0.19450072971486065\n",
            "train loss:0.3311088217229728\n",
            "train loss:0.18844410743845802\n",
            "train loss:0.14108170715715845\n",
            "=== epoch:7, train acc:0.938, test acc:0.91 ===\n",
            "train loss:0.11460518955439677\n",
            "train loss:0.13817262201662323\n",
            "train loss:0.13495897282797195\n",
            "train loss:0.21787743862445763\n",
            "train loss:0.12501650805326603\n",
            "train loss:0.14639690964275426\n",
            "train loss:0.15293065529593827\n",
            "train loss:0.14950171536301254\n",
            "train loss:0.17733304000414182\n",
            "train loss:0.14855718253118727\n",
            "train loss:0.15389984533721238\n",
            "train loss:0.16966226689137617\n",
            "train loss:0.14691846068226525\n",
            "train loss:0.11410193517753008\n",
            "train loss:0.11154591896373853\n",
            "train loss:0.210537628305035\n",
            "train loss:0.16347562738786933\n",
            "train loss:0.17814156210300017\n",
            "train loss:0.23556829970077914\n",
            "train loss:0.16426831800445868\n",
            "train loss:0.1484828932546712\n",
            "train loss:0.2241150164166452\n",
            "train loss:0.17986164598962076\n",
            "train loss:0.15460354908311807\n",
            "train loss:0.2875744052321733\n",
            "train loss:0.14500708348566876\n",
            "train loss:0.18241010486246176\n",
            "train loss:0.1492394841443347\n",
            "train loss:0.12089602380541228\n",
            "train loss:0.12142186500307012\n",
            "train loss:0.19037915742371692\n",
            "train loss:0.15902081599059606\n",
            "train loss:0.22690316524774734\n",
            "train loss:0.18173981608058637\n",
            "train loss:0.15045549077782075\n",
            "train loss:0.11438895085084479\n",
            "train loss:0.27893633467784507\n",
            "train loss:0.13629062290459712\n",
            "train loss:0.13037168283765005\n",
            "train loss:0.25079500665927656\n",
            "train loss:0.1677762413549572\n",
            "train loss:0.09363307656437335\n",
            "train loss:0.18190914506090705\n",
            "train loss:0.1543619781956905\n",
            "train loss:0.16300310373371324\n",
            "train loss:0.33625642801886046\n",
            "train loss:0.25912698158111014\n",
            "train loss:0.16226012783375623\n",
            "train loss:0.1607588601329202\n",
            "train loss:0.11773983004752417\n",
            "=== epoch:8, train acc:0.947, test acc:0.92 ===\n",
            "train loss:0.1302602857611048\n",
            "train loss:0.11572322327390955\n",
            "train loss:0.20789326087419613\n",
            "train loss:0.14702140834374966\n",
            "train loss:0.09555558265081678\n",
            "train loss:0.14192759910218686\n",
            "train loss:0.08365030869736988\n",
            "train loss:0.1650593569454275\n",
            "train loss:0.21713107197933154\n",
            "train loss:0.2468852553338433\n",
            "train loss:0.08843825757709237\n",
            "train loss:0.1452453485633205\n",
            "train loss:0.10018659470078614\n",
            "train loss:0.1014035469956677\n",
            "train loss:0.10548101405000779\n",
            "train loss:0.09424960192311684\n",
            "train loss:0.14817756901332835\n",
            "train loss:0.1524085947725444\n",
            "train loss:0.15769310495528163\n",
            "train loss:0.3227150640508046\n",
            "train loss:0.2788060230763987\n",
            "train loss:0.20647142581106467\n",
            "train loss:0.1935129388094952\n",
            "train loss:0.055699185343888805\n",
            "train loss:0.1345515925343278\n",
            "train loss:0.0938691383296294\n",
            "train loss:0.102189136258575\n",
            "train loss:0.11814262832574798\n",
            "train loss:0.19829178170860284\n",
            "train loss:0.16411482479935416\n",
            "train loss:0.11943871274417617\n",
            "train loss:0.19978913087614153\n",
            "train loss:0.2062499252446679\n",
            "train loss:0.24012465339794425\n",
            "train loss:0.20723016774413794\n",
            "train loss:0.2478509958108648\n",
            "train loss:0.13842702733407367\n",
            "train loss:0.16236211748357804\n",
            "train loss:0.11428184495560535\n",
            "train loss:0.16871650543364997\n",
            "train loss:0.12669658942198783\n",
            "train loss:0.12213396144672388\n",
            "train loss:0.07453322058062964\n",
            "train loss:0.11864957628200709\n",
            "train loss:0.16183708511409975\n",
            "train loss:0.11173412431628794\n",
            "train loss:0.1546915597695352\n",
            "train loss:0.059414019704054806\n",
            "train loss:0.19952515834099743\n",
            "train loss:0.07685959536643294\n",
            "=== epoch:9, train acc:0.956, test acc:0.932 ===\n",
            "train loss:0.15504536859854398\n",
            "train loss:0.16652933510885046\n",
            "train loss:0.164341747447966\n",
            "train loss:0.09140697439559058\n",
            "train loss:0.14195671156450526\n",
            "train loss:0.19159409501192734\n",
            "train loss:0.04943034547818315\n",
            "train loss:0.07906039991014194\n",
            "train loss:0.12167882310695616\n",
            "train loss:0.1808903332892517\n",
            "train loss:0.06676284234171727\n",
            "train loss:0.12027066022247636\n",
            "train loss:0.20093089330175193\n",
            "train loss:0.14096641876623148\n",
            "train loss:0.11851574232664479\n",
            "train loss:0.06842855948956038\n",
            "train loss:0.08842407900850242\n",
            "train loss:0.08381761399319916\n",
            "train loss:0.17840050741108462\n",
            "train loss:0.10762867187987407\n",
            "train loss:0.13765892622121048\n",
            "train loss:0.08956880898957306\n",
            "train loss:0.10737472062703096\n",
            "train loss:0.13090971522443998\n",
            "train loss:0.06920098205910627\n",
            "train loss:0.10678521340433061\n",
            "train loss:0.1377530079722188\n",
            "train loss:0.19160565157681764\n",
            "train loss:0.10398874296939317\n",
            "train loss:0.07307377522371374\n",
            "train loss:0.09927065091712206\n",
            "train loss:0.16800638936566642\n",
            "train loss:0.13056218275876832\n",
            "train loss:0.13581171624717325\n",
            "train loss:0.11786011188363839\n",
            "train loss:0.07709301075292696\n",
            "train loss:0.1639795360248616\n",
            "train loss:0.11577637096912533\n",
            "train loss:0.15868767941438724\n",
            "train loss:0.1769715319517631\n",
            "train loss:0.10556803869245909\n",
            "train loss:0.10357449177811161\n",
            "train loss:0.0865289083369877\n",
            "train loss:0.10065202492846499\n",
            "train loss:0.09474501016346909\n",
            "train loss:0.11267378089927044\n",
            "train loss:0.1705581622235532\n",
            "train loss:0.1497715397272048\n",
            "train loss:0.12901785689541143\n",
            "train loss:0.10733425103502539\n",
            "=== epoch:10, train acc:0.957, test acc:0.942 ===\n",
            "train loss:0.06077580933447917\n",
            "train loss:0.1273591070035479\n",
            "train loss:0.041325267643119314\n",
            "train loss:0.09734106090420461\n",
            "train loss:0.13516506935026534\n",
            "train loss:0.09321783194175909\n",
            "train loss:0.09226435122974717\n",
            "train loss:0.1838845269288861\n",
            "train loss:0.0748352379909359\n",
            "train loss:0.09340151357804555\n",
            "train loss:0.14353596270320806\n",
            "train loss:0.07689315307459124\n",
            "train loss:0.10842598973122451\n",
            "train loss:0.20399593764923296\n",
            "train loss:0.08380173723852513\n",
            "train loss:0.1505156500695411\n",
            "train loss:0.11544835119269817\n",
            "train loss:0.05036420235114813\n",
            "train loss:0.22285092113635052\n",
            "train loss:0.051101594476309284\n",
            "train loss:0.11470244590110708\n",
            "train loss:0.10063035509103128\n",
            "train loss:0.11020698216527386\n",
            "train loss:0.023775118078162814\n",
            "train loss:0.09359265652075946\n",
            "train loss:0.08214406611727741\n",
            "train loss:0.10662419005427727\n",
            "train loss:0.10149534460472705\n",
            "train loss:0.07540750961353519\n",
            "train loss:0.09232168673984228\n",
            "train loss:0.13582934387509477\n",
            "train loss:0.08854316578290282\n",
            "train loss:0.11116714460719518\n",
            "train loss:0.1336291212178823\n",
            "train loss:0.08452936361250604\n",
            "train loss:0.05788913299105857\n",
            "train loss:0.13943897822974388\n",
            "train loss:0.046611889778004734\n",
            "train loss:0.07721354014882251\n",
            "train loss:0.13556512920217462\n",
            "train loss:0.0903090586630965\n",
            "train loss:0.11404668250478889\n",
            "train loss:0.14885326884926328\n",
            "train loss:0.07133522184146464\n",
            "train loss:0.0874971811264834\n",
            "train loss:0.15616824845482064\n",
            "train loss:0.12214507578873954\n",
            "train loss:0.08081186138764314\n",
            "train loss:0.19708678446223948\n",
            "train loss:0.15359375820813623\n",
            "=== epoch:11, train acc:0.959, test acc:0.943 ===\n",
            "train loss:0.1149070757190687\n",
            "train loss:0.11517739266394866\n",
            "train loss:0.05868876865389058\n",
            "train loss:0.13123222738989784\n",
            "train loss:0.11788373248507908\n",
            "train loss:0.0583536549888012\n",
            "train loss:0.08604977279067817\n",
            "train loss:0.06854346754955289\n",
            "train loss:0.06508791888040975\n",
            "train loss:0.20926958351747224\n",
            "train loss:0.04335449360716561\n",
            "train loss:0.053852598142803944\n",
            "train loss:0.13771448340989229\n",
            "train loss:0.16653759471991733\n",
            "train loss:0.06789977129931336\n",
            "train loss:0.06546119343304063\n",
            "train loss:0.16460510517646532\n",
            "train loss:0.0763064111280191\n",
            "train loss:0.05287857486170622\n",
            "train loss:0.08301378477911463\n",
            "train loss:0.11671944754694426\n",
            "train loss:0.10162159426233366\n",
            "train loss:0.07535054723630308\n",
            "train loss:0.10324630420807647\n",
            "train loss:0.15705816442538734\n",
            "train loss:0.10395615399589442\n",
            "train loss:0.05490471128979788\n",
            "train loss:0.07487377290854023\n",
            "train loss:0.2524432552765719\n",
            "train loss:0.12358039540725901\n",
            "train loss:0.10656380939027037\n",
            "train loss:0.08844467103569474\n",
            "train loss:0.12270501803064782\n",
            "train loss:0.05501872228490254\n",
            "train loss:0.12020508604260095\n",
            "train loss:0.09713615549829767\n",
            "train loss:0.09571394236931681\n",
            "train loss:0.06947621508094691\n",
            "train loss:0.07637090621005511\n",
            "train loss:0.07626554824184374\n",
            "train loss:0.05198778357559702\n",
            "train loss:0.05750941789185607\n",
            "train loss:0.036096325085474994\n",
            "train loss:0.07221490708479655\n",
            "train loss:0.10297080234098253\n",
            "train loss:0.07150158806846242\n",
            "train loss:0.08759604089606621\n",
            "train loss:0.05486911335051386\n",
            "train loss:0.091476417075683\n",
            "train loss:0.13595409318156318\n",
            "=== epoch:12, train acc:0.973, test acc:0.944 ===\n",
            "train loss:0.10635070654864444\n",
            "train loss:0.0709552512463266\n",
            "train loss:0.045419001472657046\n",
            "train loss:0.18416103173220688\n",
            "train loss:0.18010367292707247\n",
            "train loss:0.17492521099303832\n",
            "train loss:0.10829583834128557\n",
            "train loss:0.0629430291347271\n",
            "train loss:0.10943159888206674\n",
            "train loss:0.09652233327536829\n",
            "train loss:0.0870745536229639\n",
            "train loss:0.03419904525306659\n",
            "train loss:0.1434392555208247\n",
            "train loss:0.0970115799499749\n",
            "train loss:0.08252461951726005\n",
            "train loss:0.06138132641169637\n",
            "train loss:0.08385671318614904\n",
            "train loss:0.11367626433787847\n",
            "train loss:0.07004631424698675\n",
            "train loss:0.043629059202893304\n",
            "train loss:0.15741648688451748\n",
            "train loss:0.08556327396876977\n",
            "train loss:0.05716448557621831\n",
            "train loss:0.205807387034754\n",
            "train loss:0.048867625496691644\n",
            "train loss:0.07203428317737347\n",
            "train loss:0.13826676502514992\n",
            "train loss:0.08497531185561343\n",
            "train loss:0.1554738306325621\n",
            "train loss:0.04780412787994861\n",
            "train loss:0.09465390952765473\n",
            "train loss:0.05121292113179523\n",
            "train loss:0.0625239007019507\n",
            "train loss:0.07907062498320698\n",
            "train loss:0.06122924423116036\n",
            "train loss:0.04867198485548693\n",
            "train loss:0.06077361774149085\n",
            "train loss:0.08552501961375969\n",
            "train loss:0.03735283266730184\n",
            "train loss:0.04130402055155901\n",
            "train loss:0.0368849168133146\n",
            "train loss:0.1743305523637043\n",
            "train loss:0.05218730717784537\n",
            "train loss:0.0760579793698189\n",
            "train loss:0.04458491104328344\n",
            "train loss:0.07772455651885266\n",
            "train loss:0.15065294587115477\n",
            "train loss:0.07836403737199955\n",
            "train loss:0.07275115482977944\n",
            "train loss:0.08858714678233508\n",
            "=== epoch:13, train acc:0.965, test acc:0.945 ===\n",
            "train loss:0.10061626816060162\n",
            "train loss:0.03962840399980795\n",
            "train loss:0.02672841010799686\n",
            "train loss:0.035125270717561524\n",
            "train loss:0.07908234147958199\n",
            "train loss:0.03249173820519213\n",
            "train loss:0.1056602760151831\n",
            "train loss:0.04206991692213964\n",
            "train loss:0.04309856024428182\n",
            "train loss:0.13457718751866035\n",
            "train loss:0.10303954365399984\n",
            "train loss:0.06945559426704373\n",
            "train loss:0.10388794789442629\n",
            "train loss:0.05890814439100396\n",
            "train loss:0.08773509437706567\n",
            "train loss:0.059043672282269165\n",
            "train loss:0.04802275590028592\n",
            "train loss:0.09358099328313849\n",
            "train loss:0.1070720015786791\n",
            "train loss:0.09399240419955143\n",
            "train loss:0.06296533145728285\n",
            "train loss:0.06576781167290058\n",
            "train loss:0.047059889787879\n",
            "train loss:0.1461098697305015\n",
            "train loss:0.07337605201455501\n",
            "train loss:0.06763044925954316\n",
            "train loss:0.11517426010277441\n",
            "train loss:0.10541795880199074\n",
            "train loss:0.07547068065636515\n",
            "train loss:0.0895325493317555\n",
            "train loss:0.09989398822073432\n",
            "train loss:0.07822291772605747\n",
            "train loss:0.08465782155656118\n",
            "train loss:0.09644381587858324\n",
            "train loss:0.1603883839821451\n",
            "train loss:0.05862058231376022\n",
            "train loss:0.10207282365589812\n",
            "train loss:0.06522097513677247\n",
            "train loss:0.0978907744108119\n",
            "train loss:0.07562069223048716\n",
            "train loss:0.0622755286284805\n",
            "train loss:0.1253743288459778\n",
            "train loss:0.07630721839958494\n",
            "train loss:0.0766366219276468\n",
            "train loss:0.08863591706272762\n",
            "train loss:0.08895943690564405\n",
            "train loss:0.03372215025927239\n",
            "train loss:0.1265468117495189\n",
            "train loss:0.1152877749791715\n",
            "train loss:0.03498336897957609\n",
            "=== epoch:14, train acc:0.974, test acc:0.95 ===\n",
            "train loss:0.1149051762365991\n",
            "train loss:0.06769605742480922\n",
            "train loss:0.08746018481731108\n",
            "train loss:0.04530119689342775\n",
            "train loss:0.04803235508784429\n",
            "train loss:0.1014661560398725\n",
            "train loss:0.06593764614057213\n",
            "train loss:0.03602670351210564\n",
            "train loss:0.08797705770948809\n",
            "train loss:0.10585763439228069\n",
            "train loss:0.09808232497578016\n",
            "train loss:0.021935295353783624\n",
            "train loss:0.048259691911019455\n",
            "train loss:0.02564186657195204\n",
            "train loss:0.05804539796764436\n",
            "train loss:0.031076353035302238\n",
            "train loss:0.056251455052433424\n",
            "train loss:0.05303189444496475\n",
            "train loss:0.1438644198126985\n",
            "train loss:0.05237715350306975\n",
            "train loss:0.04023808909087632\n",
            "train loss:0.061025022854952864\n",
            "train loss:0.047237674257466\n",
            "train loss:0.05972130020782219\n",
            "train loss:0.11017863748565514\n",
            "train loss:0.12258257827190992\n",
            "train loss:0.0748377544577536\n",
            "train loss:0.05870907628567631\n",
            "train loss:0.05373207966824136\n",
            "train loss:0.07984241265280333\n",
            "train loss:0.03857207647383498\n",
            "train loss:0.047738854181562956\n",
            "train loss:0.02706569592596128\n",
            "train loss:0.07085882678356097\n",
            "train loss:0.08699788490438966\n",
            "train loss:0.13484949327569148\n",
            "train loss:0.08186767798052118\n",
            "train loss:0.04066214475049722\n",
            "train loss:0.08190073474120671\n",
            "train loss:0.08194879400568018\n",
            "train loss:0.07113321713248832\n",
            "train loss:0.039298977500373034\n",
            "train loss:0.07071600984535138\n",
            "train loss:0.08379319493676554\n",
            "train loss:0.05645293266155353\n",
            "train loss:0.045929301039950084\n",
            "train loss:0.04297617980908314\n",
            "train loss:0.04646338482747304\n",
            "train loss:0.0806996437619604\n",
            "train loss:0.03278239947959518\n",
            "=== epoch:15, train acc:0.98, test acc:0.956 ===\n",
            "train loss:0.0611114117284109\n",
            "train loss:0.08734023616363702\n",
            "train loss:0.05348367624812085\n",
            "train loss:0.07533202774582495\n",
            "train loss:0.04544301476468255\n",
            "train loss:0.0862914897067332\n",
            "train loss:0.023078086107638125\n",
            "train loss:0.03775262353403958\n",
            "train loss:0.07409675135564103\n",
            "train loss:0.0840149052523624\n",
            "train loss:0.042109008023849114\n",
            "train loss:0.04119818285123283\n",
            "train loss:0.03267326785450956\n",
            "train loss:0.046796584194739116\n",
            "train loss:0.11439115359154794\n",
            "train loss:0.049545101993690596\n",
            "train loss:0.026237880916583675\n",
            "train loss:0.05530030398160976\n",
            "train loss:0.07165317113669681\n",
            "train loss:0.03512339974712664\n",
            "train loss:0.03739520052070948\n",
            "train loss:0.05502770126880232\n",
            "train loss:0.0730311590677954\n",
            "train loss:0.06358951439679773\n",
            "train loss:0.0771056058056659\n",
            "train loss:0.019209483350469896\n",
            "train loss:0.08004099412824404\n",
            "train loss:0.044138792935649734\n",
            "train loss:0.024316905328601465\n",
            "train loss:0.028647411060180778\n",
            "train loss:0.05786194242556899\n",
            "train loss:0.03569680782865131\n",
            "train loss:0.031992888076571496\n",
            "train loss:0.057177543852989406\n",
            "train loss:0.03982976974688109\n",
            "train loss:0.045424139161204156\n",
            "train loss:0.03549866733527303\n",
            "train loss:0.05153563361009619\n",
            "train loss:0.019165840835462123\n",
            "train loss:0.04907475802701397\n",
            "train loss:0.035292864701042155\n",
            "train loss:0.06560682115987915\n",
            "train loss:0.019968253459769828\n",
            "train loss:0.04981681618331712\n",
            "train loss:0.0413610624837657\n",
            "train loss:0.07208475816994855\n",
            "train loss:0.04302799734544581\n",
            "train loss:0.03211751196729066\n",
            "train loss:0.04991755016784058\n",
            "train loss:0.08850108427706825\n",
            "=== epoch:16, train acc:0.984, test acc:0.957 ===\n",
            "train loss:0.069379766396994\n",
            "train loss:0.009429666271199968\n",
            "train loss:0.03773376220921529\n",
            "train loss:0.05390894914737557\n",
            "train loss:0.02157968629603135\n",
            "train loss:0.04784212763517519\n",
            "train loss:0.03218824398642624\n",
            "train loss:0.026717875966148087\n",
            "train loss:0.05293090034440746\n",
            "train loss:0.05480504872172491\n",
            "train loss:0.04449305122959962\n",
            "train loss:0.049596656353592526\n",
            "train loss:0.04871547595689307\n",
            "train loss:0.06634880749269764\n",
            "train loss:0.04808303803934437\n",
            "train loss:0.04677383832354793\n",
            "train loss:0.06791355032829575\n",
            "train loss:0.034471589200255545\n",
            "train loss:0.07194838871235988\n",
            "train loss:0.022349312085361547\n",
            "train loss:0.03059738757892749\n",
            "train loss:0.055194173398051605\n",
            "train loss:0.02528050737360198\n",
            "train loss:0.03715213340457551\n",
            "train loss:0.03988122184011752\n",
            "train loss:0.021193953515392154\n",
            "train loss:0.04699108626097108\n",
            "train loss:0.040355246321092525\n",
            "train loss:0.01776357605671135\n",
            "train loss:0.03606900868968163\n",
            "train loss:0.054241783316365505\n",
            "train loss:0.051300241233606866\n",
            "train loss:0.018832466182270437\n",
            "train loss:0.036557020365987375\n",
            "train loss:0.020439511147386975\n",
            "train loss:0.021278248651207886\n",
            "train loss:0.0431485852563495\n",
            "train loss:0.04264143979746387\n",
            "train loss:0.02269108798520771\n",
            "train loss:0.049132389103249796\n",
            "train loss:0.019897130838232014\n",
            "train loss:0.07253917505501678\n",
            "train loss:0.04919120741504694\n",
            "train loss:0.04491484922957525\n",
            "train loss:0.030560492437992418\n",
            "train loss:0.04659604095439641\n",
            "train loss:0.08736254600510306\n",
            "train loss:0.05225268111338966\n",
            "train loss:0.06941193499327128\n",
            "train loss:0.04878303407230064\n",
            "=== epoch:17, train acc:0.984, test acc:0.955 ===\n",
            "train loss:0.035747897777378156\n",
            "train loss:0.03278135643106178\n",
            "train loss:0.03028407039133272\n",
            "train loss:0.029039398679033296\n",
            "train loss:0.03662841308782735\n",
            "train loss:0.032532827947511514\n",
            "train loss:0.03069851488552132\n",
            "train loss:0.029243589075674977\n",
            "train loss:0.0317201649998011\n",
            "train loss:0.056591605367744095\n",
            "train loss:0.027896210851549422\n",
            "train loss:0.04392660885905672\n",
            "train loss:0.045511956055732695\n",
            "train loss:0.03451039626973035\n",
            "train loss:0.031833664479445065\n",
            "train loss:0.07572946597113002\n",
            "train loss:0.03882074203206081\n",
            "train loss:0.029615705174696458\n",
            "train loss:0.04583700932559635\n",
            "train loss:0.05821161962615011\n",
            "train loss:0.01797099890880171\n",
            "train loss:0.06294317074895464\n",
            "train loss:0.07720870674715\n",
            "train loss:0.026165333598963426\n",
            "train loss:0.024531989602673995\n",
            "train loss:0.034796910764932056\n",
            "train loss:0.05513886117917492\n",
            "train loss:0.02107807558666486\n",
            "train loss:0.03789156044198812\n",
            "train loss:0.0364584221062358\n",
            "train loss:0.06516191595125882\n",
            "train loss:0.026027648842617226\n",
            "train loss:0.05360742660455701\n",
            "train loss:0.05737935283639673\n",
            "train loss:0.02755230579746951\n",
            "train loss:0.05350086805984356\n",
            "train loss:0.05934756407519406\n",
            "train loss:0.022934710108494\n",
            "train loss:0.049112056414720474\n",
            "train loss:0.022215223383488473\n",
            "train loss:0.032993831471526916\n",
            "train loss:0.024883727715410232\n",
            "train loss:0.03524171605451402\n",
            "train loss:0.0312778835679488\n",
            "train loss:0.034056655904376565\n",
            "train loss:0.02208222260236154\n",
            "train loss:0.04730153707378533\n",
            "train loss:0.005681132094456294\n",
            "train loss:0.021067548969405027\n",
            "train loss:0.023703187719243526\n",
            "=== epoch:18, train acc:0.986, test acc:0.956 ===\n",
            "train loss:0.02278401529500865\n",
            "train loss:0.052277457265721486\n",
            "train loss:0.03449446491143924\n",
            "train loss:0.014013318443258678\n",
            "train loss:0.0200685007874009\n",
            "train loss:0.018353978720244846\n",
            "train loss:0.037557854778264704\n",
            "train loss:0.027890415498318496\n",
            "train loss:0.03506830474437\n",
            "train loss:0.01981652076606216\n",
            "train loss:0.04067250520405169\n",
            "train loss:0.04224967666692124\n",
            "train loss:0.019517434771793397\n",
            "train loss:0.05931521328866352\n",
            "train loss:0.024048526995520673\n",
            "train loss:0.04501817170618223\n",
            "train loss:0.009877304639496803\n",
            "train loss:0.06433293698943925\n",
            "train loss:0.0450577649262818\n",
            "train loss:0.01824530948106298\n",
            "train loss:0.028673836381207015\n",
            "train loss:0.05903957529673114\n",
            "train loss:0.020342552512857836\n",
            "train loss:0.0254422938569411\n",
            "train loss:0.0429890657366512\n",
            "train loss:0.02203071470577902\n",
            "train loss:0.027316550940196804\n",
            "train loss:0.037909685182866595\n",
            "train loss:0.0435417327351082\n",
            "train loss:0.037263316973023375\n",
            "train loss:0.034075759396567526\n",
            "train loss:0.03595227491405417\n",
            "train loss:0.02720009492379768\n",
            "train loss:0.0699254016215907\n",
            "train loss:0.012790397507753118\n",
            "train loss:0.03250146500282709\n",
            "train loss:0.03754396811459194\n",
            "train loss:0.03969242757692988\n",
            "train loss:0.045807502289956024\n",
            "train loss:0.027968978949355936\n",
            "train loss:0.0439997330848538\n",
            "train loss:0.035321316854549804\n",
            "train loss:0.013624520436425954\n",
            "train loss:0.02868545041004051\n",
            "train loss:0.03467438063006955\n",
            "train loss:0.027347665674642724\n",
            "train loss:0.01636765328356997\n",
            "train loss:0.046739448376819134\n",
            "train loss:0.02605954243371096\n",
            "train loss:0.01660419208641744\n",
            "=== epoch:19, train acc:0.975, test acc:0.955 ===\n",
            "train loss:0.012490562252871889\n",
            "train loss:0.05695656977567153\n",
            "train loss:0.06453108789056727\n",
            "train loss:0.018785465991174387\n",
            "train loss:0.018640515175815928\n",
            "train loss:0.05587708212974985\n",
            "train loss:0.03568214798179296\n",
            "train loss:0.04674557754925448\n",
            "train loss:0.02217679292929734\n",
            "train loss:0.08313603760127139\n",
            "train loss:0.028030885325404594\n",
            "train loss:0.021993613174269358\n",
            "train loss:0.012681137655826789\n",
            "train loss:0.03960191120311159\n",
            "train loss:0.030023403983329557\n",
            "train loss:0.03884495143580103\n",
            "train loss:0.028668495978932215\n",
            "train loss:0.015229436583440193\n",
            "train loss:0.04654892272195869\n",
            "train loss:0.014983906680030355\n",
            "train loss:0.025664637091136807\n",
            "train loss:0.01823276158937726\n",
            "train loss:0.01861892406229483\n",
            "train loss:0.0334321671877709\n",
            "train loss:0.041453209708936685\n",
            "train loss:0.03767634806362569\n",
            "train loss:0.04500355863466076\n",
            "train loss:0.025011899773087508\n",
            "train loss:0.05248327059591996\n",
            "train loss:0.013265405087191566\n",
            "train loss:0.0151907847959949\n",
            "train loss:0.046420092461652525\n",
            "train loss:0.04106573941861809\n",
            "train loss:0.025884524560896675\n",
            "train loss:0.03898867042660441\n",
            "train loss:0.023568894939509893\n",
            "train loss:0.016883405123782738\n",
            "train loss:0.007069787014997704\n",
            "train loss:0.0252245859943127\n",
            "train loss:0.053542977647730694\n",
            "train loss:0.02221864970257058\n",
            "train loss:0.03263283218460607\n",
            "train loss:0.023357345406914826\n",
            "train loss:0.03895260166602276\n",
            "train loss:0.04478827936156101\n",
            "train loss:0.009905523553290045\n",
            "train loss:0.034590793878744544\n",
            "train loss:0.03173647952029592\n",
            "train loss:0.01741841671244833\n",
            "train loss:0.04644715518818463\n",
            "=== epoch:20, train acc:0.988, test acc:0.964 ===\n",
            "train loss:0.01935724099629088\n",
            "train loss:0.03612619746233992\n",
            "train loss:0.02556516075833071\n",
            "train loss:0.029171961757541047\n",
            "train loss:0.02096949808697116\n",
            "train loss:0.037962828749575066\n",
            "train loss:0.01951187856228105\n",
            "train loss:0.01766241822390745\n",
            "train loss:0.019611252901450472\n",
            "train loss:0.027083003295827354\n",
            "train loss:0.02752355045766127\n",
            "train loss:0.06017592401369362\n",
            "train loss:0.009450464688531154\n",
            "train loss:0.014323996182318745\n",
            "train loss:0.01587763252560023\n",
            "train loss:0.023133724426622707\n",
            "train loss:0.04433529116384773\n",
            "train loss:0.021740029310965085\n",
            "train loss:0.025323735749188482\n",
            "train loss:0.009429854120285025\n",
            "train loss:0.021105880049206615\n",
            "train loss:0.036387092652044546\n",
            "train loss:0.012483311423701886\n",
            "train loss:0.016480661820265627\n",
            "train loss:0.01312713640271397\n",
            "train loss:0.024600182183634006\n",
            "train loss:0.012704157022981544\n",
            "train loss:0.017799856640606692\n",
            "train loss:0.018370445377424386\n",
            "train loss:0.005904643010174861\n",
            "train loss:0.010385811699831504\n",
            "train loss:0.04636448488505837\n",
            "train loss:0.034975517462172\n",
            "train loss:0.018653216411308876\n",
            "train loss:0.02335420447242962\n",
            "train loss:0.01508685657953915\n",
            "train loss:0.006798917392835231\n",
            "train loss:0.012842811449221283\n",
            "train loss:0.023080861496487762\n",
            "train loss:0.024585316309983604\n",
            "train loss:0.018015159131247925\n",
            "train loss:0.02464173566599999\n",
            "train loss:0.007962786014638075\n",
            "train loss:0.014856611796994752\n",
            "train loss:0.017118532334893865\n",
            "train loss:0.005295460383229499\n",
            "train loss:0.007199474036950796\n",
            "train loss:0.019039215801843173\n",
            "train loss:0.019027855173103972\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.9632\n",
            "Saved Network Parameters!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tQRhjuX1AZ1o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}