{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "basic_deeplearning_7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1zo_WK68Ofs5gXxYNloefKz7mGr-2mxSa",
      "authorship_tag": "ABX9TyNcoHCteu5Supo7K8uau5XB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkworldchampion/Military_CodingStudy/blob/main/deeplearning/basic_deeplearning_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 합성곱 신경망(CNN)"
      ],
      "metadata": {
        "id": "Jif5qm3pqiku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번 장의 주제는 합성곱 신경망입니다.  \n",
        " CNN은 이미지 인식과 음성 인식 등 다양한 곳에서 사용되는데,   \n",
        " 특히 이미지 인식 분야에서 딥러닝을 활용한 기법은 대부분 CNN을 기반으로 합니다.   \n",
        " 이번장에서는 CNN의 매커니즘을 자세히 알아봅시다."
      ],
      "metadata": {
        "id": "eSc6hgZAqhx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1 전체 구조\n",
        "CNN도 지금까지와 같이 레고 블럭처럼 조합하여 만들 수 있습니다. 다만, 합성곱 계층과 풀링계층이 새롭게 등장합니다. 지금까지의 신경망은 인접하는 계층의 모든 뉴런과 결합되어 있습니다. 이를 완전연결(Fully-Connected)이라고 하며, Affine 계층이라는 이름으로 구현했습니다. 완전연결 신경망은 Affine 계층 뒤에 활성화 함수를 갖는 ReLU 계층이 이어집니다.  \n",
        "즉, 이러한 연결이 Conv-ReLU-(Pooling)가 되었다고 생각하면 됩니다. "
      ],
      "metadata": {
        "id": "iF8fMsLYrACw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM1mT5yvs9hL",
        "outputId": "40465123-51cc-4815-ec0c-4dae4e3ac13c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Colab_Notebooks/Machine_learning/book/basic_deeplearning/deep-learning-from-scratch/ch07"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3CDWzNJtB1p",
        "outputId": "7f7d7c29-165a-4b3f-a13d-2a4d433f4280"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab_Notebooks/Machine_learning/book/basic_deeplearning/deep-learning-from-scratch/ch07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from simple_convnet import SimpleConvNet\n",
        "from matplotlib.image import imread\n",
        "from common.layers import *\n",
        "import pickle\n",
        "from collections import OrderedDict\n",
        "from common.gradient import numerical_gradient\n",
        "from dataset.mnist import load_mnist\n",
        "from simple_convnet import SimpleConvNet\n",
        "from common.trainer import Trainer"
      ],
      "metadata": {
        "id": "-yuWfeuhs35j"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# im2col 함수 구현해보기\n",
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    col : 2차원 배열\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_data.shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1   # 출력 크기 구하는 공식\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1   \n",
        "\n",
        "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')   # data를 pad만큼 둘러싼 모양으로 된다. \n",
        "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "    return col"
      ],
      "metadata": {
        "id": "K6amaSkIrMmC"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filter_h = 5\n",
        "filter_w = 5\n",
        "stride = 1\n",
        "pad = 0\n",
        "\n",
        "\n",
        "input_data = np.random.rand(1, 3, 7, 7).round(2)\n",
        "N, C, H, W = input_data.shape\n",
        "out_h = (H + 2*pad - filter_h)//stride + 1   # 3\n",
        "out_w = (W + 2*pad - filter_w)//stride + 1   # 3\n",
        "img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')  # pad-0이기 때문에 원본데이터\n",
        "col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))  # np.zeros((1, 3, 5, 5, 3, 3))\n",
        "num = 0\n",
        "for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            print(num, y_max, x_max)\n",
        "            num+=1\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]  # col은 필터, img는 전체 이미지\n",
        "# 여기까지, col은 이미지가 합쳐질 필터를 저장해놓은 공간, img는 전체 이미지 데이터를 저장하는 공간"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fYBSHUVr3OD",
        "outputId": "56d963c9-f807-4ddd-bfb8-a25f463e7d67"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 3 3\n",
            "1 3 4\n",
            "2 3 5\n",
            "3 3 6\n",
            "4 3 7\n",
            "5 4 3\n",
            "6 4 4\n",
            "7 4 5\n",
            "8 4 6\n",
            "9 4 7\n",
            "10 5 3\n",
            "11 5 4\n",
            "12 5 5\n",
            "13 5 6\n",
            "14 5 7\n",
            "15 6 3\n",
            "16 6 4\n",
            "17 6 5\n",
            "18 6 6\n",
            "19 6 7\n",
            "20 7 3\n",
            "21 7 4\n",
            "22 7 5\n",
            "23 7 6\n",
            "24 7 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 행렬에 대한 transpose() 함수에 대한 고찰\n",
        "test_col = np.arange(24).reshape(2, 3, 4)\n",
        "print(test_col)\n",
        "print(\"---------------------\")\n",
        "print(test_col.transpose(0, 2, 1))\n",
        "print(\"---------------------\")\n",
        "print(test_col.transpose(2, 1, 0))\n",
        "# 그림과 함께 보고싶다면 \"https://jimmy-ai.tistory.com/207\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6dmFun_mu2j",
        "outputId": "53837cd0-c52b-4da0-c15e-7ac9ba530203"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 0  1  2  3]\n",
            "  [ 4  5  6  7]\n",
            "  [ 8  9 10 11]]\n",
            "\n",
            " [[12 13 14 15]\n",
            "  [16 17 18 19]\n",
            "  [20 21 22 23]]]\n",
            "---------------------\n",
            "[[[ 0  4  8]\n",
            "  [ 1  5  9]\n",
            "  [ 2  6 10]\n",
            "  [ 3  7 11]]\n",
            "\n",
            " [[12 16 20]\n",
            "  [13 17 21]\n",
            "  [14 18 22]\n",
            "  [15 19 23]]]\n",
            "---------------------\n",
            "[[[ 0 12]\n",
            "  [ 4 16]\n",
            "  [ 8 20]]\n",
            "\n",
            " [[ 1 13]\n",
            "  [ 5 17]\n",
            "  [ 9 21]]\n",
            "\n",
            " [[ 2 14]\n",
            "  [ 6 18]\n",
            "  [10 22]]\n",
            "\n",
            " [[ 3 15]\n",
            "  [ 7 19]\n",
            "  [11 23]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#col_last = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)  # 이 줄이 어떻게 동작하나 알아보자\n",
        "print(col.shape) # 우선 원래의 행렬\n",
        "print(col.transpose(0, 4, 5, 1, 2, 3).shape) # 바뀐 행렬 (1, 3, 3, 3, 5, 5)로 바뀜\n",
        "print(col.reshape(N*out_h*out_w, -1))  # N*out_h*out_w 의 숫자와 , 나머지 남은 수"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nrBQAkLyHja",
        "outputId": "69d8c879-4e7f-4e02-9078-f57e779f80e5"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 3, 5, 5, 3, 3)\n",
            "(1, 3, 3, 3, 5, 5)\n",
            "[[0.4  0.08 0.44 0.63 0.88 0.62 0.84 0.23 0.53 0.08 0.44 0.99 0.88 0.62\n",
            "  0.86 0.23 0.53 0.13 0.44 0.99 0.52 0.62 0.86 0.19 0.53 0.13 0.23 0.99\n",
            "  0.52 0.66 0.86 0.19 0.35 0.13 0.23 0.76 0.52 0.66 0.05 0.19 0.35 0.73\n",
            "  0.23 0.76 0.12 0.63 0.88 0.62 0.84 0.23 0.53 0.12 0.65 0.69 0.88 0.62\n",
            "  0.86 0.23 0.53 0.13 0.65 0.69 0.82 0.62 0.86 0.19 0.53 0.13 0.23 0.69\n",
            "  0.82 0.11 0.86 0.19 0.35]\n",
            " [0.13 0.23 0.76 0.82 0.11 0.   0.19 0.35 0.73 0.23 0.76 0.12 0.11 0.\n",
            "  0.61 0.84 0.23 0.53 0.12 0.65 0.69 0.2  0.98 0.14 0.23 0.53 0.13 0.65\n",
            "  0.69 0.82 0.98 0.14 0.93 0.53 0.13 0.23 0.69 0.82 0.11 0.14 0.93 0.03\n",
            "  0.13 0.23 0.76 0.82 0.11 0.   0.93 0.03 0.18 0.23 0.76 0.12 0.11 0.\n",
            "  0.61 0.03 0.18 0.9  0.12 0.65 0.69 0.2  0.98 0.14 0.12 0.8  0.1  0.65\n",
            "  0.69 0.82 0.98 0.14 0.93]\n",
            " [0.8  0.1  0.29 0.69 0.82 0.11 0.14 0.93 0.03 0.1  0.29 0.5  0.82 0.11\n",
            "  0.   0.93 0.03 0.18 0.29 0.5  0.49 0.11 0.   0.61 0.03 0.18 0.9  0.5\n",
            "  0.49 0.35 0.2  0.98 0.14 0.12 0.8  0.1  0.2  0.48 0.58 0.98 0.14 0.93\n",
            "  0.8  0.1  0.29 0.48 0.58 0.04 0.14 0.93 0.03 0.1  0.29 0.5  0.58 0.04\n",
            "  0.37 0.93 0.03 0.18 0.29 0.5  0.49 0.04 0.37 0.79 0.03 0.18 0.9  0.5\n",
            "  0.49 0.35 0.37 0.79 0.7 ]\n",
            " [0.61 0.11 0.61 0.44 0.68 0.37 0.97 0.49 0.45 0.11 0.61 0.49 0.68 0.37\n",
            "  0.37 0.49 0.45 0.93 0.61 0.49 0.22 0.37 0.37 0.54 0.45 0.93 0.68 0.49\n",
            "  0.22 0.97 0.37 0.54 0.26 0.93 0.68 0.86 0.22 0.97 0.69 0.54 0.26 0.45\n",
            "  0.68 0.86 0.56 0.44 0.68 0.37 0.97 0.49 0.45 0.86 0.49 0.61 0.68 0.37\n",
            "  0.37 0.49 0.45 0.93 0.49 0.61 0.41 0.37 0.37 0.54 0.45 0.93 0.68 0.61\n",
            "  0.41 0.95 0.37 0.54 0.26]\n",
            " [0.93 0.68 0.86 0.41 0.95 0.29 0.54 0.26 0.45 0.68 0.86 0.56 0.95 0.29\n",
            "  0.52 0.97 0.49 0.45 0.86 0.49 0.61 0.34 0.92 0.21 0.49 0.45 0.93 0.49\n",
            "  0.61 0.41 0.92 0.21 0.21 0.45 0.93 0.68 0.61 0.41 0.95 0.21 0.21 0.31\n",
            "  0.93 0.68 0.86 0.41 0.95 0.29 0.21 0.31 0.38 0.68 0.86 0.56 0.95 0.29\n",
            "  0.52 0.31 0.38 0.66 0.86 0.49 0.61 0.34 0.92 0.21 0.21 0.38 0.26 0.49\n",
            "  0.61 0.41 0.92 0.21 0.21]\n",
            " [0.38 0.26 0.83 0.61 0.41 0.95 0.21 0.21 0.31 0.26 0.83 0.5  0.41 0.95\n",
            "  0.29 0.21 0.31 0.38 0.83 0.5  0.34 0.95 0.29 0.52 0.31 0.38 0.66 0.5\n",
            "  0.34 0.76 0.34 0.92 0.21 0.21 0.38 0.26 0.08 0.05 0.23 0.92 0.21 0.21\n",
            "  0.38 0.26 0.83 0.05 0.23 0.74 0.21 0.21 0.31 0.26 0.83 0.5  0.23 0.74\n",
            "  0.41 0.21 0.31 0.38 0.83 0.5  0.34 0.74 0.41 0.72 0.31 0.38 0.66 0.5\n",
            "  0.34 0.76 0.41 0.72 0.85]\n",
            " [0.99 0.21 0.58 0.46 0.7  0.2  0.52 0.63 0.18 0.21 0.58 0.67 0.7  0.2\n",
            "  0.81 0.63 0.18 0.76 0.58 0.67 0.25 0.2  0.81 0.43 0.18 0.76 0.64 0.67\n",
            "  0.25 0.2  0.81 0.43 0.22 0.76 0.64 0.09 0.25 0.2  0.78 0.43 0.22 0.5\n",
            "  0.64 0.09 0.75 0.46 0.7  0.2  0.52 0.63 0.18 0.64 0.18 0.61 0.7  0.2\n",
            "  0.81 0.63 0.18 0.76 0.18 0.61 0.33 0.2  0.81 0.43 0.18 0.76 0.64 0.61\n",
            "  0.33 0.58 0.81 0.43 0.22]\n",
            " [0.76 0.64 0.09 0.33 0.58 0.28 0.43 0.22 0.5  0.64 0.09 0.75 0.58 0.28\n",
            "  0.52 0.52 0.63 0.18 0.64 0.18 0.61 0.03 0.57 0.17 0.63 0.18 0.76 0.18\n",
            "  0.61 0.33 0.57 0.17 0.25 0.18 0.76 0.64 0.61 0.33 0.58 0.17 0.25 0.44\n",
            "  0.76 0.64 0.09 0.33 0.58 0.28 0.25 0.44 0.   0.64 0.09 0.75 0.58 0.28\n",
            "  0.52 0.44 0.   0.75 0.64 0.18 0.61 0.03 0.57 0.17 0.38 0.53 0.44 0.18\n",
            "  0.61 0.33 0.57 0.17 0.25]\n",
            " [0.53 0.44 0.42 0.61 0.33 0.58 0.17 0.25 0.44 0.44 0.42 0.3  0.33 0.58\n",
            "  0.28 0.25 0.44 0.   0.42 0.3  0.45 0.58 0.28 0.52 0.44 0.   0.75 0.3\n",
            "  0.45 0.01 0.03 0.57 0.17 0.38 0.53 0.44 0.22 0.49 0.33 0.57 0.17 0.25\n",
            "  0.53 0.44 0.42 0.49 0.33 0.84 0.17 0.25 0.44 0.44 0.42 0.3  0.33 0.84\n",
            "  0.41 0.25 0.44 0.   0.42 0.3  0.45 0.84 0.41 0.37 0.44 0.   0.75 0.3\n",
            "  0.45 0.01 0.41 0.37 0.03]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# im2col사용해 행렬 펼치기\n",
        "from common.util import im2col\n",
        "x1 = np.random.rand(1, 3, 7, 7).round(2)\n",
        "print(x1.shape)\n",
        "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
        "print(col1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLV-IKHZAlOr",
        "outputId": "fc4edb0e-dabe-4a6b-c6e4-ca6905eab4d9"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 3, 7, 7)\n",
            "(9, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = np.random.rand(10, 3, 7, 7).round(2)\n",
        "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
        "print(col2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV5NiI5rFZDK",
        "outputId": "6574e353-f761-40b8-cf2d-d6156cce53f9"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(90, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이러한 im2col을 갖고, 합성곱 계층을 구현해보기"
      ],
      "metadata": {
        "id": "75-VRXrJsPB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 컨볼루션 신경망 구현하기\n",
        "class Convolution:\n",
        "    def __init__(self, W, b, stride=1, pad=0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 +(H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T  # 필터 전개\n",
        "        out = np.dot(col, col_W) + self.b\n",
        "\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "LbDrewMLsNtP"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 풀링 구현하기\n",
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        # 전개 (1)\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        # 최댓값 (2)\n",
        "        out = np.max(col, axis=1)\n",
        "\n",
        "        # 성형 (3)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "wdc2C1FNzLxN"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleConvNet:\n",
        "    \"\"\"단순한 합성곱 신경망\n",
        "    \n",
        "    conv - relu - pool - affine - relu - affine - softmax\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
        "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
        "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
        "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
        "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
        "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
        "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(1, 28, 28),  # 784\n",
        "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1  # output size 계산 공식\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))  \n",
        "\n",
        "        # 가중치 매개변수 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()   # 순서가 있는 딕셔너리\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                           conv_param['stride'], conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \"\"\"손실 함수를 구한다.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        \"\"\"\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        \n",
        "        acc = 0.0\n",
        "        \n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt) \n",
        "        \n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다（수치미분）\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        loss_w = lambda w: self.loss(x, t)\n",
        "        \n",
        "        grads = {}\n",
        "        for idx in (1, 2, 3):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다(오차역전파법).\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "        \n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
        "            self.layers[key].W = self.params['W' + str(i+1)]\n",
        "            self.layers[key].b = self.params['b' + str(i+1)]\n"
      ],
      "metadata": {
        "id": "AT1SDylktkze"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
        "x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28), \n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "                        \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# 매개변수 보존\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "07-RaFBZ84Cb",
        "outputId": "11496b54-a743-49a7-a682-dc7c1755ace2"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss:2.299793644481677\n",
            "=== epoch:1, train acc:0.373, test acc:0.364 ===\n",
            "train loss:2.2972841636650254\n",
            "train loss:2.2938666224710715\n",
            "train loss:2.2841361845338795\n",
            "train loss:2.2799994150979526\n",
            "train loss:2.2609564539854365\n",
            "train loss:2.25370930627701\n",
            "train loss:2.2341193299857873\n",
            "train loss:2.2145533380797784\n",
            "train loss:2.1762455659288684\n",
            "train loss:2.135684602821615\n",
            "train loss:2.1095849226808117\n",
            "train loss:2.0584600142185714\n",
            "train loss:2.023987412125722\n",
            "train loss:1.9567390899293196\n",
            "train loss:1.8964372097596012\n",
            "train loss:1.810030286602599\n",
            "train loss:1.6984510944543343\n",
            "train loss:1.650326426788798\n",
            "train loss:1.5195735070082907\n",
            "train loss:1.5445569924549911\n",
            "train loss:1.450255221236302\n",
            "train loss:1.2521764064572296\n",
            "train loss:1.2362741995361577\n",
            "train loss:1.2264139974295682\n",
            "train loss:1.0259149221037454\n",
            "train loss:1.0458306819784386\n",
            "train loss:1.0509910703145757\n",
            "train loss:0.9406811367186008\n",
            "train loss:0.8623919481983434\n",
            "train loss:0.940622041492469\n",
            "train loss:0.7139219225446528\n",
            "train loss:0.7297536873027072\n",
            "train loss:0.575560658639455\n",
            "train loss:0.5696471682560535\n",
            "train loss:0.587337172136063\n",
            "train loss:0.8033535001321388\n",
            "train loss:0.5664293481732455\n",
            "train loss:0.7593436743126456\n",
            "train loss:0.7274393163058354\n",
            "train loss:0.5405572993793998\n",
            "train loss:0.46183417147477457\n",
            "train loss:0.7069824139555465\n",
            "train loss:0.6027711242451117\n",
            "train loss:0.6031872191362143\n",
            "train loss:0.6645482713971886\n",
            "train loss:0.4771629540602243\n",
            "train loss:0.48867137534025545\n",
            "train loss:0.5216641162912904\n",
            "train loss:0.504240131975154\n",
            "train loss:0.4594718750958591\n",
            "=== epoch:2, train acc:0.828, test acc:0.797 ===\n",
            "train loss:0.3822909075523823\n",
            "train loss:0.31116686062086296\n",
            "train loss:0.57674149454796\n",
            "train loss:0.4139202169152352\n",
            "train loss:0.4153276971844127\n",
            "train loss:0.4267251707076061\n",
            "train loss:0.5047597950626151\n",
            "train loss:0.6021925290503267\n",
            "train loss:0.44761726953873393\n",
            "train loss:0.6078641324401386\n",
            "train loss:0.5043141640738487\n",
            "train loss:0.3416903621079712\n",
            "train loss:0.47680700732642944\n",
            "train loss:0.6145105612915802\n",
            "train loss:0.46176314541904334\n",
            "train loss:0.48672760483992844\n",
            "train loss:0.3757157586256148\n",
            "train loss:0.44691835609353103\n",
            "train loss:0.4592297511457828\n",
            "train loss:0.35511982784278345\n",
            "train loss:0.43363551013867974\n",
            "train loss:0.3415264816634931\n",
            "train loss:0.33189143643427954\n",
            "train loss:0.5752507676647507\n",
            "train loss:0.45741595138330626\n",
            "train loss:0.27792360476324957\n",
            "train loss:0.23700598578005094\n",
            "train loss:0.7181695859000112\n",
            "train loss:0.3016024103518411\n",
            "train loss:0.252901173547118\n",
            "train loss:0.4593115084259012\n",
            "train loss:0.49886850875360816\n",
            "train loss:0.5412762396757007\n",
            "train loss:0.43719364976202135\n",
            "train loss:0.3837591631165628\n",
            "train loss:0.40272926738208187\n",
            "train loss:0.4047455993888954\n",
            "train loss:0.2759874856657264\n",
            "train loss:0.38909093913239284\n",
            "train loss:0.5595446123689272\n",
            "train loss:0.45306475913174793\n",
            "train loss:0.44362119841797265\n",
            "train loss:0.46632321094040685\n",
            "train loss:0.4988656237508076\n",
            "train loss:0.30739784131227654\n",
            "train loss:0.33785985625272724\n",
            "train loss:0.3190747889750365\n",
            "train loss:0.43655510101546463\n",
            "train loss:0.3982354864578194\n",
            "train loss:0.33493987597483155\n",
            "=== epoch:3, train acc:0.872, test acc:0.865 ===\n",
            "train loss:0.3949163324777684\n",
            "train loss:0.263861106379705\n",
            "train loss:0.32413551109553046\n",
            "train loss:0.3708017922744641\n",
            "train loss:0.26146937859684516\n",
            "train loss:0.29792514203738113\n",
            "train loss:0.4887815756763355\n",
            "train loss:0.46525740556024997\n",
            "train loss:0.339716588990975\n",
            "train loss:0.29358677639984476\n",
            "train loss:0.20751187878564611\n",
            "train loss:0.3746066225895482\n",
            "train loss:0.32362185327158743\n",
            "train loss:0.3298448352343095\n",
            "train loss:0.2503233403069674\n",
            "train loss:0.435667919805401\n",
            "train loss:0.3375104288112932\n",
            "train loss:0.2877995594273764\n",
            "train loss:0.39736445137182713\n",
            "train loss:0.4937851136829318\n",
            "train loss:0.28362303007124856\n",
            "train loss:0.23509812684737588\n",
            "train loss:0.5081076315818449\n",
            "train loss:0.18744192463043652\n",
            "train loss:0.23433926001268682\n",
            "train loss:0.2677531844202312\n",
            "train loss:0.3319606220600953\n",
            "train loss:0.2741700936614108\n",
            "train loss:0.23480880436349227\n",
            "train loss:0.32236672410685097\n",
            "train loss:0.24605972289833436\n",
            "train loss:0.21454814610830286\n",
            "train loss:0.37330581764814746\n",
            "train loss:0.5023636564486853\n",
            "train loss:0.4601146991251366\n",
            "train loss:0.3800934493430135\n",
            "train loss:0.271027576650142\n",
            "train loss:0.3203961577599387\n",
            "train loss:0.23298771140592123\n",
            "train loss:0.2202655871080806\n",
            "train loss:0.17760007655999133\n",
            "train loss:0.43444679037776346\n",
            "train loss:0.2536035210167577\n",
            "train loss:0.36599346457181925\n",
            "train loss:0.19375859199033718\n",
            "train loss:0.49709019336503785\n",
            "train loss:0.25815001811693206\n",
            "train loss:0.28461965020978697\n",
            "train loss:0.2252025009705832\n",
            "train loss:0.1395281396845942\n",
            "=== epoch:4, train acc:0.888, test acc:0.883 ===\n",
            "train loss:0.23932946864688556\n",
            "train loss:0.3049332572506269\n",
            "train loss:0.16306291967997283\n",
            "train loss:0.19474468605912265\n",
            "train loss:0.24727204397513197\n",
            "train loss:0.24743622903785528\n",
            "train loss:0.2563212030450439\n",
            "train loss:0.2522650530650237\n",
            "train loss:0.2032724258227882\n",
            "train loss:0.3874824105597197\n",
            "train loss:0.21334625104958715\n",
            "train loss:0.32828853322151275\n",
            "train loss:0.2982900549817321\n",
            "train loss:0.4894747526977586\n",
            "train loss:0.44620110878268915\n",
            "train loss:0.2758016699300277\n",
            "train loss:0.2726289433852237\n",
            "train loss:0.17720715335851342\n",
            "train loss:0.25697693333637195\n",
            "train loss:0.20686717844554764\n",
            "train loss:0.28797187724573925\n",
            "train loss:0.18410341876183411\n",
            "train loss:0.24479207345550358\n",
            "train loss:0.3685618273671174\n",
            "train loss:0.19273702114130561\n",
            "train loss:0.260435867564017\n",
            "train loss:0.1872288899540024\n",
            "train loss:0.38093194513082534\n",
            "train loss:0.3402740709010585\n",
            "train loss:0.31861498760930274\n",
            "train loss:0.20555919240876677\n",
            "train loss:0.2121870364219466\n",
            "train loss:0.20601244774159766\n",
            "train loss:0.23958729214086744\n",
            "train loss:0.2705939165190662\n",
            "train loss:0.41538469690834484\n",
            "train loss:0.16528047220282754\n",
            "train loss:0.18216485501126276\n",
            "train loss:0.24470167168092588\n",
            "train loss:0.4144004112856571\n",
            "train loss:0.2351619771592027\n",
            "train loss:0.263094737756769\n",
            "train loss:0.27226398742148417\n",
            "train loss:0.26132164432388594\n",
            "train loss:0.36206922255012175\n",
            "train loss:0.3244402859248594\n",
            "train loss:0.26835171769013233\n",
            "train loss:0.17371871220641125\n",
            "train loss:0.1946308231331939\n",
            "train loss:0.2781191348536007\n",
            "=== epoch:5, train acc:0.911, test acc:0.892 ===\n",
            "train loss:0.16724771873592936\n",
            "train loss:0.18947747584339308\n",
            "train loss:0.15815880238504312\n",
            "train loss:0.16060710951649415\n",
            "train loss:0.2209952769314165\n",
            "train loss:0.19049375906334834\n",
            "train loss:0.251322804102962\n",
            "train loss:0.1532309947657022\n",
            "train loss:0.2651883438843497\n",
            "train loss:0.2465591031517929\n",
            "train loss:0.1740949704434854\n",
            "train loss:0.22122455697237967\n",
            "train loss:0.26054622117548304\n",
            "train loss:0.2203542139680208\n",
            "train loss:0.1480213617044455\n",
            "train loss:0.17030724794485072\n",
            "train loss:0.13431363192631038\n",
            "train loss:0.223992207682896\n",
            "train loss:0.2352893928935992\n",
            "train loss:0.2793278415143019\n",
            "train loss:0.17026545255260936\n",
            "train loss:0.22336060453393858\n",
            "train loss:0.2046250677707835\n",
            "train loss:0.30521652986414016\n",
            "train loss:0.14869481735527953\n",
            "train loss:0.16244804777743707\n",
            "train loss:0.17055879525295262\n",
            "train loss:0.19565989085316282\n",
            "train loss:0.2759624503882611\n",
            "train loss:0.338821771054185\n",
            "train loss:0.18202911514058265\n",
            "train loss:0.19424406480775075\n",
            "train loss:0.16026341790917975\n",
            "train loss:0.16874706563342026\n",
            "train loss:0.25828169376660354\n",
            "train loss:0.13902645739094474\n",
            "train loss:0.26559666926863057\n",
            "train loss:0.17598031257498822\n",
            "train loss:0.3076532281995425\n",
            "train loss:0.09875753959476181\n",
            "train loss:0.2052081549682033\n",
            "train loss:0.14608733336284385\n",
            "train loss:0.22157545111097854\n",
            "train loss:0.17287365264555965\n",
            "train loss:0.16705867684983308\n",
            "train loss:0.16255085128335545\n",
            "train loss:0.11839533557336564\n",
            "train loss:0.189781398601921\n",
            "train loss:0.20745591605310115\n",
            "train loss:0.15462577888383827\n",
            "=== epoch:6, train acc:0.929, test acc:0.899 ===\n",
            "train loss:0.12250171913068639\n",
            "train loss:0.25415114017262774\n",
            "train loss:0.20155389284726674\n",
            "train loss:0.18931778789967782\n",
            "train loss:0.08765004788425385\n",
            "train loss:0.10859330516604056\n",
            "train loss:0.09189392596232457\n",
            "train loss:0.19639056200062857\n",
            "train loss:0.32838548167211173\n",
            "train loss:0.18359231896045553\n",
            "train loss:0.247708326887495\n",
            "train loss:0.23635943266876536\n",
            "train loss:0.1984623170310789\n",
            "train loss:0.4611249277145124\n",
            "train loss:0.23142403618579388\n",
            "train loss:0.38337736260032024\n",
            "train loss:0.14111065730385097\n",
            "train loss:0.12698628062260964\n",
            "train loss:0.1446337284910745\n",
            "train loss:0.13399956271416189\n",
            "train loss:0.20657874441622284\n",
            "train loss:0.20802477258692892\n",
            "train loss:0.14938761397883707\n",
            "train loss:0.14035275063902086\n",
            "train loss:0.08027603477963734\n",
            "train loss:0.2704091979917658\n",
            "train loss:0.2110527730244806\n",
            "train loss:0.08553194736920647\n",
            "train loss:0.17871813533390613\n",
            "train loss:0.1905231516045816\n",
            "train loss:0.18480453771149835\n",
            "train loss:0.1981091910918961\n",
            "train loss:0.21822805261012074\n",
            "train loss:0.17761593912506457\n",
            "train loss:0.1774487965935368\n",
            "train loss:0.22370255645157403\n",
            "train loss:0.2667420014897802\n",
            "train loss:0.19302479068990075\n",
            "train loss:0.25259084035306545\n",
            "train loss:0.259767049784817\n",
            "train loss:0.12717046548568164\n",
            "train loss:0.19466643133088327\n",
            "train loss:0.15670135780650135\n",
            "train loss:0.11553079706537325\n",
            "train loss:0.15442071765612186\n",
            "train loss:0.15027671156118688\n",
            "train loss:0.14857781901163952\n",
            "train loss:0.1168974521899621\n",
            "train loss:0.13619586861504385\n",
            "train loss:0.09755002170615108\n",
            "=== epoch:7, train acc:0.938, test acc:0.921 ===\n",
            "train loss:0.08625429550387884\n",
            "train loss:0.2797274977045711\n",
            "train loss:0.14044307321182642\n",
            "train loss:0.245900730481141\n",
            "train loss:0.14862583103777222\n",
            "train loss:0.22935082202337223\n",
            "train loss:0.19287242278391975\n",
            "train loss:0.17490688658821044\n",
            "train loss:0.17142215647512085\n",
            "train loss:0.14923168279591242\n",
            "train loss:0.4484231853989168\n",
            "train loss:0.21148155592522283\n",
            "train loss:0.12501667473813263\n",
            "train loss:0.10421077150495318\n",
            "train loss:0.09901494927691826\n",
            "train loss:0.2572006208568731\n",
            "train loss:0.14268313107334998\n",
            "train loss:0.1827440612275611\n",
            "train loss:0.1312092832152717\n",
            "train loss:0.1908490501491694\n",
            "train loss:0.12625066691433307\n",
            "train loss:0.12484983347820966\n",
            "train loss:0.1326429491034844\n",
            "train loss:0.07934465365314003\n",
            "train loss:0.1366250795889251\n",
            "train loss:0.38255151951145516\n",
            "train loss:0.17321681163460861\n",
            "train loss:0.09549333253908587\n",
            "train loss:0.1564692687199544\n",
            "train loss:0.18393161664197544\n",
            "train loss:0.1410902670357433\n",
            "train loss:0.17328099113932524\n",
            "train loss:0.16729023951839594\n",
            "train loss:0.1642901464268799\n",
            "train loss:0.17397054200187287\n",
            "train loss:0.09122983963150752\n",
            "train loss:0.10926850495914384\n",
            "train loss:0.2321885734081249\n",
            "train loss:0.22633788417946066\n",
            "train loss:0.16806323509594093\n",
            "train loss:0.2042948009198243\n",
            "train loss:0.20159944102588515\n",
            "train loss:0.11275569831328289\n",
            "train loss:0.17038710915818672\n",
            "train loss:0.233772417363134\n",
            "train loss:0.22773995079334283\n",
            "train loss:0.1279384262172293\n",
            "train loss:0.15513332237497357\n",
            "train loss:0.12387931079697705\n",
            "train loss:0.12740317396372475\n",
            "=== epoch:8, train acc:0.949, test acc:0.922 ===\n",
            "train loss:0.1309103996754486\n",
            "train loss:0.19282167797872382\n",
            "train loss:0.09839387185140629\n",
            "train loss:0.14467911124766525\n",
            "train loss:0.1722692015810892\n",
            "train loss:0.32064587713068027\n",
            "train loss:0.19298440976494752\n",
            "train loss:0.10406822919897346\n",
            "train loss:0.07222409868412279\n",
            "train loss:0.16967656697573127\n",
            "train loss:0.13876791663277457\n",
            "train loss:0.1641760201570326\n",
            "train loss:0.057919120071444556\n",
            "train loss:0.15489172179018879\n",
            "train loss:0.05788683487565132\n",
            "train loss:0.15068511959624387\n",
            "train loss:0.09973756258756138\n",
            "train loss:0.1260117412804572\n",
            "train loss:0.1015170749555518\n",
            "train loss:0.11726204251602726\n",
            "train loss:0.06678236069893524\n",
            "train loss:0.12049890135557989\n",
            "train loss:0.1567741023909832\n",
            "train loss:0.24710895056316748\n",
            "train loss:0.18421695801182533\n",
            "train loss:0.125544603942593\n",
            "train loss:0.06825746995472279\n",
            "train loss:0.0910020895393233\n",
            "train loss:0.2024326274924022\n",
            "train loss:0.09796248741443138\n",
            "train loss:0.12323161719771696\n",
            "train loss:0.08477729893660145\n",
            "train loss:0.2143892552079212\n",
            "train loss:0.08979317734737625\n",
            "train loss:0.17576566058581417\n",
            "train loss:0.0838238480816347\n",
            "train loss:0.11878708394921761\n",
            "train loss:0.18299848515381922\n",
            "train loss:0.10008267087041842\n",
            "train loss:0.11536517632746664\n",
            "train loss:0.12301322715777924\n",
            "train loss:0.07504357780797317\n",
            "train loss:0.13332773626042668\n",
            "train loss:0.11447072132989498\n",
            "train loss:0.12777346669903228\n",
            "train loss:0.09198439901191743\n",
            "train loss:0.09928779103053528\n",
            "train loss:0.06458958449142989\n",
            "train loss:0.13090119984896145\n",
            "train loss:0.17929226251057404\n",
            "=== epoch:9, train acc:0.953, test acc:0.928 ===\n",
            "train loss:0.13210977589268733\n",
            "train loss:0.1379413728938998\n",
            "train loss:0.09138984355108624\n",
            "train loss:0.060917383909187085\n",
            "train loss:0.11906176034211312\n",
            "train loss:0.27976200086462716\n",
            "train loss:0.1475707361293924\n",
            "train loss:0.05001516985016575\n",
            "train loss:0.16442089191475984\n",
            "train loss:0.10892161524386099\n",
            "train loss:0.14725191502624546\n",
            "train loss:0.05488368398581355\n",
            "train loss:0.07478523855588126\n",
            "train loss:0.10187191304593558\n",
            "train loss:0.1997659542253081\n",
            "train loss:0.19824177614119734\n",
            "train loss:0.16919387968406532\n",
            "train loss:0.11305454440162807\n",
            "train loss:0.20737828386718846\n",
            "train loss:0.07621529532954384\n",
            "train loss:0.09254382199196279\n",
            "train loss:0.1463688946608462\n",
            "train loss:0.10498644565345377\n",
            "train loss:0.10792293113690933\n",
            "train loss:0.15728577873436453\n",
            "train loss:0.07688577186576294\n",
            "train loss:0.19082296294074427\n",
            "train loss:0.06327606092252179\n",
            "train loss:0.10747284459268892\n",
            "train loss:0.1031882770071189\n",
            "train loss:0.1625677206295019\n",
            "train loss:0.08773148917221456\n",
            "train loss:0.1393812122996222\n",
            "train loss:0.11259232278429307\n",
            "train loss:0.08275499766521004\n",
            "train loss:0.1377171420355939\n",
            "train loss:0.11549079097492154\n",
            "train loss:0.09574872246952858\n",
            "train loss:0.06489874062307627\n",
            "train loss:0.13881307525526576\n",
            "train loss:0.05977192396716054\n",
            "train loss:0.12879192281809348\n",
            "train loss:0.09091372499957381\n",
            "train loss:0.1247506833376914\n",
            "train loss:0.162167502154035\n",
            "train loss:0.08338478261764674\n",
            "train loss:0.04717917907215538\n",
            "train loss:0.17796044155895413\n",
            "train loss:0.0898248106550427\n",
            "train loss:0.10966917856699294\n",
            "=== epoch:10, train acc:0.957, test acc:0.943 ===\n",
            "train loss:0.08009517641951852\n",
            "train loss:0.06205414018438013\n",
            "train loss:0.07521283511015675\n",
            "train loss:0.047951926899428246\n",
            "train loss:0.17350688992438393\n",
            "train loss:0.11740127242582542\n",
            "train loss:0.07208768965020307\n",
            "train loss:0.11278915191970482\n",
            "train loss:0.09491066394300895\n",
            "train loss:0.06632110384555816\n",
            "train loss:0.08535547289869257\n",
            "train loss:0.1727842131792784\n",
            "train loss:0.09053737326258517\n",
            "train loss:0.07623437240399969\n",
            "train loss:0.08571922649856564\n",
            "train loss:0.06045885718633851\n",
            "train loss:0.046601951145422864\n",
            "train loss:0.14117960228456403\n",
            "train loss:0.12334273931613422\n",
            "train loss:0.11875617862797194\n",
            "train loss:0.10420434642776065\n",
            "train loss:0.07689370542545125\n",
            "train loss:0.031185419844866665\n",
            "train loss:0.05154535579789022\n",
            "train loss:0.053401269453013364\n",
            "train loss:0.049994442844294794\n",
            "train loss:0.10811250842933465\n",
            "train loss:0.06698784936459604\n",
            "train loss:0.10801297445996064\n",
            "train loss:0.06244123250812585\n",
            "train loss:0.09745171862684089\n",
            "train loss:0.07763599754482108\n",
            "train loss:0.10435992314551353\n",
            "train loss:0.09966898023565066\n",
            "train loss:0.2514366097419578\n",
            "train loss:0.08685581212354387\n",
            "train loss:0.10259991210689685\n",
            "train loss:0.12481176767334104\n",
            "train loss:0.12760852242603826\n",
            "train loss:0.0765731326620715\n",
            "train loss:0.10073951640270322\n",
            "train loss:0.13695793083970606\n",
            "train loss:0.07042458926698321\n",
            "train loss:0.16106575289163996\n",
            "train loss:0.11303308666898836\n",
            "train loss:0.06644711133898412\n",
            "train loss:0.0980795008787443\n",
            "train loss:0.14609799669581147\n",
            "train loss:0.10592525352813126\n",
            "train loss:0.06103786097283593\n",
            "=== epoch:11, train acc:0.965, test acc:0.94 ===\n",
            "train loss:0.09795819001869005\n",
            "train loss:0.11684778011541722\n",
            "train loss:0.06727674028524308\n",
            "train loss:0.06911956426304262\n",
            "train loss:0.07852065794396068\n",
            "train loss:0.14294005334560828\n",
            "train loss:0.08683772627237013\n",
            "train loss:0.03523911308421985\n",
            "train loss:0.06703284747463506\n",
            "train loss:0.07412303921035163\n",
            "train loss:0.0851051057795146\n",
            "train loss:0.09267599781529265\n",
            "train loss:0.05004842818835037\n",
            "train loss:0.0338622559840613\n",
            "train loss:0.07381548743759779\n",
            "train loss:0.09359622439647643\n",
            "train loss:0.07981817671143526\n",
            "train loss:0.07404821965921718\n",
            "train loss:0.052850774640051376\n",
            "train loss:0.0898379172611261\n",
            "train loss:0.045466162334450166\n",
            "train loss:0.029986554716040413\n",
            "train loss:0.08443350758201555\n",
            "train loss:0.10032128107548614\n",
            "train loss:0.03148455792377735\n",
            "train loss:0.03152771958244664\n",
            "train loss:0.10792180879917072\n",
            "train loss:0.10252770501453756\n",
            "train loss:0.06050868276388159\n",
            "train loss:0.05076786310586032\n",
            "train loss:0.06319654375333289\n",
            "train loss:0.1421748382632235\n",
            "train loss:0.11093413230453944\n",
            "train loss:0.02845376717963858\n",
            "train loss:0.04850776405908604\n",
            "train loss:0.08685515829167141\n",
            "train loss:0.06788366491338264\n",
            "train loss:0.09993077209561578\n",
            "train loss:0.04154806956975779\n",
            "train loss:0.05148797333399262\n",
            "train loss:0.07466732630828052\n",
            "train loss:0.03561387487279103\n",
            "train loss:0.07254727445540013\n",
            "train loss:0.149505138428479\n",
            "train loss:0.03706399481328691\n",
            "train loss:0.1214462347227671\n",
            "train loss:0.041469948838111115\n",
            "train loss:0.0904231294494891\n",
            "train loss:0.05213253724552522\n",
            "train loss:0.05535305594077908\n",
            "=== epoch:12, train acc:0.968, test acc:0.947 ===\n",
            "train loss:0.07459245792552491\n",
            "train loss:0.06240591643998607\n",
            "train loss:0.07576991068059244\n",
            "train loss:0.04871022261590521\n",
            "train loss:0.04055319459490746\n",
            "train loss:0.17789526811226192\n",
            "train loss:0.07421024595700888\n",
            "train loss:0.08709255220348146\n",
            "train loss:0.0769973585142171\n",
            "train loss:0.04991527778927802\n",
            "train loss:0.07491860020714898\n",
            "train loss:0.06959360671425908\n",
            "train loss:0.0827370400244391\n",
            "train loss:0.1070576188673493\n",
            "train loss:0.07507771191419978\n",
            "train loss:0.06960888614930283\n",
            "train loss:0.07405726979327286\n",
            "train loss:0.06328085631373469\n",
            "train loss:0.0816049276469829\n",
            "train loss:0.05526909258134915\n",
            "train loss:0.08745804007679153\n",
            "train loss:0.12425318330227754\n",
            "train loss:0.04621410654072667\n",
            "train loss:0.050132711122496794\n",
            "train loss:0.10146158620632763\n",
            "train loss:0.10134079720304114\n",
            "train loss:0.06923142390784827\n",
            "train loss:0.09555260311477617\n",
            "train loss:0.09534714131093654\n",
            "train loss:0.09567329679414018\n",
            "train loss:0.03239491022708776\n",
            "train loss:0.02745896659229741\n",
            "train loss:0.04935163856472625\n",
            "train loss:0.03660191677045162\n",
            "train loss:0.04461516353435023\n",
            "train loss:0.0695247246500254\n",
            "train loss:0.06246985409571817\n",
            "train loss:0.038076505870482516\n",
            "train loss:0.04193908236264253\n",
            "train loss:0.06513412073787726\n",
            "train loss:0.049766306475970634\n",
            "train loss:0.10736607463583345\n",
            "train loss:0.06928693190263538\n",
            "train loss:0.03920647761428834\n",
            "train loss:0.05183414474129236\n",
            "train loss:0.06966841970214799\n",
            "train loss:0.06879714638211673\n",
            "train loss:0.037621564478858026\n",
            "train loss:0.03589083654579117\n",
            "train loss:0.05087686082651621\n",
            "=== epoch:13, train acc:0.979, test acc:0.946 ===\n",
            "train loss:0.05236547000055514\n",
            "train loss:0.03791590935892972\n",
            "train loss:0.04281312670648454\n",
            "train loss:0.02564790123009453\n",
            "train loss:0.020199465288188518\n",
            "train loss:0.11746404310880731\n",
            "train loss:0.049705474663444875\n",
            "train loss:0.07635797643531977\n",
            "train loss:0.06047176862145436\n",
            "train loss:0.09360210929713184\n",
            "train loss:0.09713703594116145\n",
            "train loss:0.1059529419822948\n",
            "train loss:0.09011963274439458\n",
            "train loss:0.08013370942778185\n",
            "train loss:0.03372721794775263\n",
            "train loss:0.04977159458283671\n",
            "train loss:0.0655229971603209\n",
            "train loss:0.07706892664756139\n",
            "train loss:0.035813241229698416\n",
            "train loss:0.054182308123535645\n",
            "train loss:0.040231763856685455\n",
            "train loss:0.02646190873501343\n",
            "train loss:0.024691328837517817\n",
            "train loss:0.05169044246590257\n",
            "train loss:0.061080146666960905\n",
            "train loss:0.11571486839500589\n",
            "train loss:0.09867190677948239\n",
            "train loss:0.03291694104569062\n",
            "train loss:0.06892404191651559\n",
            "train loss:0.038051457221446296\n",
            "train loss:0.04605068792631572\n",
            "train loss:0.06629318365603037\n",
            "train loss:0.04211835352589131\n",
            "train loss:0.06474076628649278\n",
            "train loss:0.07056453392616255\n",
            "train loss:0.060066108228124\n",
            "train loss:0.027730227927041554\n",
            "train loss:0.04165779691597653\n",
            "train loss:0.0576279274310205\n",
            "train loss:0.032534774556996854\n",
            "train loss:0.11555250359706795\n",
            "train loss:0.0394354112858488\n",
            "train loss:0.08646246803838185\n",
            "train loss:0.03350361844083943\n",
            "train loss:0.048656023313328134\n",
            "train loss:0.04419881941392048\n",
            "train loss:0.03156276143415695\n",
            "train loss:0.03990778274387816\n",
            "train loss:0.020895910122841307\n",
            "train loss:0.08370364048066209\n",
            "=== epoch:14, train acc:0.982, test acc:0.946 ===\n",
            "train loss:0.026468156423488733\n",
            "train loss:0.07229760299014115\n",
            "train loss:0.022052850737021443\n",
            "train loss:0.07632384689462798\n",
            "train loss:0.07365229227444339\n",
            "train loss:0.02796054786576994\n",
            "train loss:0.039211693477189044\n",
            "train loss:0.07596496025714106\n",
            "train loss:0.019964060796156858\n",
            "train loss:0.04492388103521814\n",
            "train loss:0.08153556853303337\n",
            "train loss:0.027248928140718606\n",
            "train loss:0.07744922882025157\n",
            "train loss:0.04515408285595655\n",
            "train loss:0.021672014596250545\n",
            "train loss:0.07972949972734941\n",
            "train loss:0.07995689385984155\n",
            "train loss:0.04385138766042626\n",
            "train loss:0.049347430501107584\n",
            "train loss:0.038388666674935984\n",
            "train loss:0.028622551553641316\n",
            "train loss:0.038948824338108014\n",
            "train loss:0.024187147853375283\n",
            "train loss:0.010821679245461726\n",
            "train loss:0.036990985748078485\n",
            "train loss:0.01581146299916188\n",
            "train loss:0.026785087106756297\n",
            "train loss:0.0726407543596339\n",
            "train loss:0.022774047043923967\n",
            "train loss:0.03167270216808116\n",
            "train loss:0.021680610694533562\n",
            "train loss:0.04259591399934741\n",
            "train loss:0.06072573218038693\n",
            "train loss:0.06783672064139061\n",
            "train loss:0.030286703420974788\n",
            "train loss:0.021146764461624133\n",
            "train loss:0.040531753940679115\n",
            "train loss:0.06339910917974949\n",
            "train loss:0.029091799290917198\n",
            "train loss:0.030941276675887964\n",
            "train loss:0.13008574216182106\n",
            "train loss:0.01119429507421992\n",
            "train loss:0.02904281247657715\n",
            "train loss:0.035910268831254165\n",
            "train loss:0.06519348344132471\n",
            "train loss:0.08546400375612304\n",
            "train loss:0.06260371532876077\n",
            "train loss:0.042798464705180485\n",
            "train loss:0.026729168481273137\n",
            "train loss:0.01569977943896813\n",
            "=== epoch:15, train acc:0.987, test acc:0.957 ===\n",
            "train loss:0.03972970709256328\n",
            "train loss:0.030344477512930262\n",
            "train loss:0.012217414015138497\n",
            "train loss:0.05021207604532646\n",
            "train loss:0.02514337188952967\n",
            "train loss:0.05374242642861093\n",
            "train loss:0.08472310023470099\n",
            "train loss:0.05617983969030447\n",
            "train loss:0.07239483501008212\n",
            "train loss:0.013503353601482401\n",
            "train loss:0.04988922118399406\n",
            "train loss:0.12484213906089224\n",
            "train loss:0.04600771659515141\n",
            "train loss:0.03703397600581272\n",
            "train loss:0.03408436677286233\n",
            "train loss:0.03466612572985257\n",
            "train loss:0.11580854758593366\n",
            "train loss:0.027046387971623155\n",
            "train loss:0.01766058399243257\n",
            "train loss:0.04763392373263641\n",
            "train loss:0.03225933740483537\n",
            "train loss:0.022105238575499425\n",
            "train loss:0.019162782926685794\n",
            "train loss:0.032202365892612723\n",
            "train loss:0.04121194148161659\n",
            "train loss:0.12564503704407176\n",
            "train loss:0.020779732336797925\n",
            "train loss:0.03311608198836721\n",
            "train loss:0.023984466989413086\n",
            "train loss:0.0284418018801638\n",
            "train loss:0.039283984816250984\n",
            "train loss:0.02913503613074018\n",
            "train loss:0.021324770191635475\n",
            "train loss:0.04900502181199582\n",
            "train loss:0.03412096811777031\n",
            "train loss:0.025553939332412498\n",
            "train loss:0.024217685242194614\n",
            "train loss:0.028665134566021776\n",
            "train loss:0.029048892219485634\n",
            "train loss:0.05600898474018631\n",
            "train loss:0.06735167956613336\n",
            "train loss:0.014824713463526793\n",
            "train loss:0.03082150348937626\n",
            "train loss:0.03385484696952449\n",
            "train loss:0.03340199516706056\n",
            "train loss:0.031896290820641354\n",
            "train loss:0.05389595199170362\n",
            "train loss:0.02138110556885721\n",
            "train loss:0.01928307192389952\n",
            "train loss:0.07159477383840751\n",
            "=== epoch:16, train acc:0.981, test acc:0.948 ===\n",
            "train loss:0.061633523846529245\n",
            "train loss:0.03096182471537122\n",
            "train loss:0.03598362589765518\n",
            "train loss:0.04217238530728704\n",
            "train loss:0.05425675954721684\n",
            "train loss:0.04189784403548602\n",
            "train loss:0.07757576265523554\n",
            "train loss:0.025294209470088834\n",
            "train loss:0.15647928319536505\n",
            "train loss:0.02619524852582729\n",
            "train loss:0.031029512736919233\n",
            "train loss:0.02049891080160615\n",
            "train loss:0.11808746380079832\n",
            "train loss:0.04633837107781389\n",
            "train loss:0.04227414250385644\n",
            "train loss:0.018320561301714475\n",
            "train loss:0.025427841766827494\n",
            "train loss:0.039201499797284226\n",
            "train loss:0.05189396792598056\n",
            "train loss:0.028575431580198752\n",
            "train loss:0.0613568643097618\n",
            "train loss:0.0725280423626742\n",
            "train loss:0.056784346589595434\n",
            "train loss:0.05756600287322326\n",
            "train loss:0.06368025363144508\n",
            "train loss:0.017586155883293444\n",
            "train loss:0.05510300511458641\n",
            "train loss:0.06977063007566583\n",
            "train loss:0.027648701105326415\n",
            "train loss:0.036065377971752494\n",
            "train loss:0.05227044948443351\n",
            "train loss:0.03787375752434877\n",
            "train loss:0.013496827420514841\n",
            "train loss:0.02262690995137938\n",
            "train loss:0.031128558388311457\n",
            "train loss:0.016844008280626023\n",
            "train loss:0.023758351460980418\n",
            "train loss:0.037017774788386595\n",
            "train loss:0.03673022517740194\n",
            "train loss:0.03829036410450175\n",
            "train loss:0.04317316367177343\n",
            "train loss:0.04589468067467657\n",
            "train loss:0.02413558235715918\n",
            "train loss:0.04219016009878046\n",
            "train loss:0.021878691175337136\n",
            "train loss:0.013921136643357956\n",
            "train loss:0.043697655474996884\n",
            "train loss:0.020216862384434875\n",
            "train loss:0.022150045348633306\n",
            "train loss:0.023176669642717255\n",
            "=== epoch:17, train acc:0.992, test acc:0.959 ===\n",
            "train loss:0.018301489052858718\n",
            "train loss:0.032949858604793325\n",
            "train loss:0.014969759366329929\n",
            "train loss:0.037678990193631454\n",
            "train loss:0.024116019806862173\n",
            "train loss:0.04405536969137635\n",
            "train loss:0.025989769957757373\n",
            "train loss:0.04035266365761966\n",
            "train loss:0.0683099537249108\n",
            "train loss:0.02391009534669518\n",
            "train loss:0.07661322722141038\n",
            "train loss:0.023294148313737283\n",
            "train loss:0.027450981052903133\n",
            "train loss:0.04518268521147631\n",
            "train loss:0.055050302980353544\n",
            "train loss:0.034782754101570265\n",
            "train loss:0.05949252628992749\n",
            "train loss:0.015644449850067788\n",
            "train loss:0.035138989124254116\n",
            "train loss:0.08748428639229829\n",
            "train loss:0.026593559443911084\n",
            "train loss:0.04705144593367045\n",
            "train loss:0.03199787140333628\n",
            "train loss:0.06034697782488508\n",
            "train loss:0.010367774240801793\n",
            "train loss:0.0181833993335837\n",
            "train loss:0.019164052778905406\n",
            "train loss:0.027389701333182317\n",
            "train loss:0.034872955166047145\n",
            "train loss:0.011955640622611978\n",
            "train loss:0.06279216972888134\n",
            "train loss:0.05101157780516307\n",
            "train loss:0.03106252012414266\n",
            "train loss:0.037828445719312716\n",
            "train loss:0.011368220337031783\n",
            "train loss:0.03224095211185382\n",
            "train loss:0.023296108244821027\n",
            "train loss:0.05762754145359941\n",
            "train loss:0.025086393654758555\n",
            "train loss:0.013923614853339308\n",
            "train loss:0.027146290682000144\n",
            "train loss:0.06444374272663217\n",
            "train loss:0.015528259441020587\n",
            "train loss:0.017358535509081435\n",
            "train loss:0.05870186899633339\n",
            "train loss:0.043939106045840746\n",
            "train loss:0.016368009746798497\n",
            "train loss:0.03948656160398413\n",
            "train loss:0.012413048065509826\n",
            "train loss:0.009223127038306233\n",
            "=== epoch:18, train acc:0.989, test acc:0.96 ===\n",
            "train loss:0.011140015049965906\n",
            "train loss:0.006979802910284322\n",
            "train loss:0.013340054858740548\n",
            "train loss:0.02744994624153679\n",
            "train loss:0.03379138343115824\n",
            "train loss:0.026934988769642603\n",
            "train loss:0.023100316607230945\n",
            "train loss:0.014473922314757255\n",
            "train loss:0.02784223964752404\n",
            "train loss:0.07769531712056192\n",
            "train loss:0.017728959001556417\n",
            "train loss:0.007713693164489898\n",
            "train loss:0.014876389001158502\n",
            "train loss:0.0426056997146909\n",
            "train loss:0.012091235442609205\n",
            "train loss:0.013320631602777284\n",
            "train loss:0.020453545773053885\n",
            "train loss:0.010309641727816652\n",
            "train loss:0.014780836915310342\n",
            "train loss:0.02224457836480493\n",
            "train loss:0.048880991463350155\n",
            "train loss:0.021832841008230006\n",
            "train loss:0.011706158740479618\n",
            "train loss:0.060229747896456576\n",
            "train loss:0.015954065012335827\n",
            "train loss:0.02851877012091378\n",
            "train loss:0.034378150058316875\n",
            "train loss:0.023047791057036547\n",
            "train loss:0.047619689731970415\n",
            "train loss:0.01979724855907307\n",
            "train loss:0.0171444219029226\n",
            "train loss:0.043734596905639835\n",
            "train loss:0.01974373592257439\n",
            "train loss:0.008466375020620379\n",
            "train loss:0.014250636162118793\n",
            "train loss:0.023511832808620955\n",
            "train loss:0.03310135093202263\n",
            "train loss:0.014125966337619911\n",
            "train loss:0.012757659607316497\n",
            "train loss:0.02433346422130112\n",
            "train loss:0.014544469122331977\n",
            "train loss:0.006141953824097574\n",
            "train loss:0.022439879546991195\n",
            "train loss:0.024142180308287343\n",
            "train loss:0.02106056222490073\n",
            "train loss:0.01643478398994465\n",
            "train loss:0.015967382661930863\n",
            "train loss:0.014502708979056193\n",
            "train loss:0.009102973961655086\n",
            "train loss:0.051194932716504195\n",
            "=== epoch:19, train acc:0.993, test acc:0.961 ===\n",
            "train loss:0.02057928733694934\n",
            "train loss:0.030262933657226685\n",
            "train loss:0.00776894871737214\n",
            "train loss:0.016044604538506112\n",
            "train loss:0.019762861070965577\n",
            "train loss:0.012503848961727399\n",
            "train loss:0.035932277418311676\n",
            "train loss:0.015493291836105991\n",
            "train loss:0.01053990568801337\n",
            "train loss:0.05275128535175949\n",
            "train loss:0.04344351487881106\n",
            "train loss:0.013575283273220795\n",
            "train loss:0.012812027389330643\n",
            "train loss:0.013786575529318983\n",
            "train loss:0.023263778494920864\n",
            "train loss:0.02935736077622471\n",
            "train loss:0.020699693142879654\n",
            "train loss:0.009733162643412901\n",
            "train loss:0.03183356138763631\n",
            "train loss:0.0077125593557824355\n",
            "train loss:0.009957962799897663\n",
            "train loss:0.024830263251348676\n",
            "train loss:0.009114533236188693\n",
            "train loss:0.017285806672788093\n",
            "train loss:0.04011465617273764\n",
            "train loss:0.025192791497796404\n",
            "train loss:0.022858368614470236\n",
            "train loss:0.017068414897213124\n",
            "train loss:0.020764373018454886\n",
            "train loss:0.025398774324673523\n",
            "train loss:0.014775332599111903\n",
            "train loss:0.03887951867050593\n",
            "train loss:0.01735482645214835\n",
            "train loss:0.016645340760706083\n",
            "train loss:0.030049716370272775\n",
            "train loss:0.023045049293453964\n",
            "train loss:0.018439984346153182\n",
            "train loss:0.01291956166813759\n",
            "train loss:0.022859307528120466\n",
            "train loss:0.025307607597366392\n",
            "train loss:0.015062840752334938\n",
            "train loss:0.032605904378904815\n",
            "train loss:0.016189494967491423\n",
            "train loss:0.03073098278751651\n",
            "train loss:0.01760314636622221\n",
            "train loss:0.019394739138553604\n",
            "train loss:0.01104437486515589\n",
            "train loss:0.022806172443311495\n",
            "train loss:0.004374004733465548\n",
            "train loss:0.016232156643539306\n",
            "=== epoch:20, train acc:0.993, test acc:0.96 ===\n",
            "train loss:0.010433328125915737\n",
            "train loss:0.009688889308291553\n",
            "train loss:0.012274055673512117\n",
            "train loss:0.010020094604747825\n",
            "train loss:0.0201074590931874\n",
            "train loss:0.014794142212596775\n",
            "train loss:0.016688910664890165\n",
            "train loss:0.008315197399394146\n",
            "train loss:0.05770470058311163\n",
            "train loss:0.0069359401647146315\n",
            "train loss:0.0063222690124706\n",
            "train loss:0.016385648670298573\n",
            "train loss:0.012226054150421559\n",
            "train loss:0.016158945628130136\n",
            "train loss:0.01023876128713707\n",
            "train loss:0.017333774875350195\n",
            "train loss:0.016621796946357396\n",
            "train loss:0.01140330529922373\n",
            "train loss:0.024125311232824374\n",
            "train loss:0.021484737609066346\n",
            "train loss:0.011402748128361008\n",
            "train loss:0.017994892920401802\n",
            "train loss:0.014720354855066236\n",
            "train loss:0.02060652329221997\n",
            "train loss:0.008376565139869067\n",
            "train loss:0.02264976334643827\n",
            "train loss:0.016944647704566555\n",
            "train loss:0.014190568474609574\n",
            "train loss:0.023830491591314663\n",
            "train loss:0.007134067514233026\n",
            "train loss:0.006694936666075656\n",
            "train loss:0.011565274149900157\n",
            "train loss:0.029129353232153866\n",
            "train loss:0.029619498353229003\n",
            "train loss:0.1021861532761364\n",
            "train loss:0.020391539618800275\n",
            "train loss:0.011418581531850136\n",
            "train loss:0.009222365063999604\n",
            "train loss:0.006426196600027404\n",
            "train loss:0.021739014647704197\n",
            "train loss:0.015256632066235983\n",
            "train loss:0.03702469331562985\n",
            "train loss:0.013039307093580265\n",
            "train loss:0.016174705065087946\n",
            "train loss:0.00818334784056468\n",
            "train loss:0.01207593816660989\n",
            "train loss:0.03817547527721533\n",
            "train loss:0.007589145543546659\n",
            "train loss:0.041391655029311976\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.9639\n",
            "Saved Network Parameters!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN 시각화 하기"
      ],
      "metadata": {
        "id": "yLhJQqwV_QRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1번째 층의 가중치 시각화하기\n",
        "def filter_show(filters, nx=8, margin=3, scale=10):\n",
        "    \"\"\"\n",
        "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
        "    \"\"\"\n",
        "    FN, C, FH, FW = filters.shape\n",
        "    ny = int(np.ceil(FN / nx))\n",
        "\n",
        "    fig = plt.figure()\n",
        "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "    for i in range(FN):\n",
        "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
        "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "network = SimpleConvNet()\n",
        "# 무작위(랜덤) 초기화 후의 가중치\n",
        "filter_show(network.params['W1'])\n",
        "\n",
        "# 학습된 가중치\n",
        "network.load_params(\"params.pkl\")\n",
        "filter_show(network.params['W1'])"
      ],
      "metadata": {
        "id": "tQRhjuX1AZ1o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "outputId": "75e1d40e-3837-44c7-e32b-616f8e92958a"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 30 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 30 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}